{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fL-T2SY0YNJltcGWMzdq5173sMxfEQhV",
      "authorship_tag": "ABX9TyM2niHYMo6cq+5l0lA5JqPE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdey26/DataSense_AI_Agent/blob/main/DataSense_AI_Agent_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bNsqpOUjP4d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, sys\n",
        "\n",
        "# If the notebook is run from a repo clone in /content or from Drive, set REPO_ROOT appropriately.\n",
        "# Users can override REPO_ROOT env var if needed.\n",
        "repo_root = os.getenv(\"REPO_ROOT\", os.path.abspath(\".\"))\n",
        "\n",
        "# If running in Colab and the repo is cloned to /content/DataSense_AI_Agent,\n",
        "# users can set REPO_ROOT environment to that path before running the notebook:\n",
        "# %env REPO_ROOT=/content/DataSense_AI_Agent\n",
        "\n",
        "# Add repo root to sys.path so \"from services...\" imports work\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "print(\"Using repo_root:\", repo_root)\n",
        "# Create a portable OUTPUT_DIR for artifacts\n",
        "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"/tmp/datasense_outputs\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "CUC_cPjiiEnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes if you get the error while running the One-cell then just Cell 3 and\n",
        "then restart the session.                                                  \n",
        "(Runtime -> Restart Session or Ctrl + M )\n"
      ],
      "metadata": {
        "id": "sDP5qhn5PuWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîµ ONE-CELL DEMO with interactive upload (Colab)\n",
        "from IPython.display import FileLink, display\n",
        "import time, os, sys\n",
        "from google.colab import files   # works only in Colab\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Explicitly set repo_root to the expected path of the DataSense_AI_Agent repository\n",
        "repo_root = \"/content/DataSense_AI_Agent\" # Changed line\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "from services.session_service import SessionService\n",
        "from utils.logger import AgentLogger\n",
        "from services.memory_bank import MemoryBank\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from agents.reporter_agent import ReporterAgent\n",
        "from config import Config\n",
        "\n",
        "# Ensure OUTPUT_DIR exists (if not defined earlier)\n",
        "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"/tmp/datasense_outputs\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Starting ONE-CELL FULL PIPELINE DEMO (Upload or fallback to sample)\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session_service = SessionService()\n",
        "memory = MemoryBank()\n",
        "\n",
        "print(f\"Session created: {session_service.session_id}\")\n",
        "print(\"Memory Bank initialized.\\n\")\n",
        "\n",
        "# --- Upload prompt ---\n",
        "print(\"üîΩ Upload a CSV file now (or cancel to use bundled sample):\")\n",
        "uploaded = files.upload()   # opens file picker in Colab\n",
        "\n",
        "if uploaded:\n",
        "    # take the first uploaded file\n",
        "    uploaded_name = list(uploaded.keys())[0]\n",
        "    saved_path = os.path.join(OUTPUT_DIR, uploaded_name)\n",
        "    # write bytes to OUTPUT_DIR\n",
        "    with open(saved_path, \"wb\") as f:\n",
        "        f.write(uploaded[uploaded_name])\n",
        "    sample_file_path = saved_path\n",
        "    print(f\"Uploaded file saved to: {sample_file_path}\")\n",
        "else:\n",
        "    # fallback to sample included in the repo\n",
        "    # This path assumes Config.BASE_PATH is correctly defined within the DataSense_AI_Agent project.\n",
        "    sample_file_path = f\"{Config.BASE_PATH}/sample_data/sales.csv\"\n",
        "    print(\"No file uploaded. Falling back to bundled sample:\", sample_file_path)\n",
        "\n",
        "# Validate path\n",
        "if not os.path.exists(sample_file_path):\n",
        "    raise FileNotFoundError(f\"Dataset path not found: {sample_file_path}\")\n",
        "\n",
        "# --- Pipeline Steps ---\n",
        "print(\"\\n============================================================\")\n",
        "print(\"Step 1: Running Ingest Agent...\")\n",
        "print(\"============================================================\")\n",
        "start_ingest = time.time()\n",
        "ingest_agent = IngestAgent(session_service, logger)\n",
        "# Try to use ingest_csv(path). If your IngestAgent expects a DataFrame, read and call ingest_dataframe.\n",
        "try:\n",
        "    profile = ingest_agent.ingest_csv(sample_file_path)\n",
        "except AttributeError:\n",
        "    # fallback: read as pandas and call ingest_dataframe if available\n",
        "    import pandas as pd\n",
        "    df_local = pd.read_csv(sample_file_path)\n",
        "    if hasattr(ingest_agent, \"ingest_dataframe\"):\n",
        "        profile = ingest_agent.ingest_dataframe(df_local)\n",
        "    else:\n",
        "        # as last resort, store df in session and continue\n",
        "        session_service.store(session_service.session_id, \"df\", df_local)\n",
        "        profile = {\"filename\": os.path.basename(sample_file_path), \"quality_metrics\": {\"completeness_score\": None, \"overall_quality\": None}}\n",
        "        print(\"Warning: IngestAgent.ingest_csv not available; stored DataFrame directly in session.\")\n",
        "\n",
        "df = ingest_agent.get_dataframe() if hasattr(ingest_agent, \"get_dataframe\") else session_service.get(session_service.session_id, \"df\")\n",
        "ingest_time = time.time() - start_ingest\n",
        "print(f\"Ingestion done ({ingest_time:.2f}s). Dataset: {profile.get('filename')} ({df.shape[0]} rows)\")\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\"Step 2: Running Analysis Agent...\")\n",
        "print(\"============================================================\")\n",
        "start_analysis = time.time()\n",
        "analysis_agent_instance = AnalysisAgent(session_service, logger)\n",
        "analysis_res = analysis_agent_instance.analyze(df)\n",
        "analysis_time = time.time() - start_analysis\n",
        "print(f\"Analysis done ({analysis_time:.2f}s). Insights: {len(analysis_res.get('insights', []))}, Visualizations: {len(analysis_res.get('visualizations', []))}\")\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\"Step 3: Running Anomaly Agent...\")\n",
        "print(\"============================================================\")\n",
        "start_anomaly = time.time()\n",
        "anomaly_agent_instance = AnomalyAgent(session_service, logger)\n",
        "anomaly_res = anomaly_agent_instance.detect_anomalies(df)\n",
        "anomaly_time = time.time() - start_anomaly\n",
        "print(f\"Anomaly detection done ({anomaly_time:.2f}s). Total Anomalies: {anomaly_res.get('total_anomalies', 0)}\")\n",
        "\n",
        "# Store analysis in memory\n",
        "analysis_summary = {\n",
        "    'shape': df.shape,\n",
        "    'columns': df.columns.tolist(),\n",
        "    'anomalies_count': anomaly_res.get('total_anomalies', 0),\n",
        "    'quality_score': profile.get('quality_metrics', {}).get('completeness_score'),\n",
        "    'key_findings': [\n",
        "        f\"Detected {anomaly_res.get('total_anomalies', 0)} anomalies\",\n",
        "        f\"Quality: {profile.get('quality_metrics', {}).get('overall_quality')}\",\n",
        "        f\"Generated {len(analysis_res.get('visualizations', []))} visualizations\"\n",
        "    ]\n",
        "}\n",
        "memory_id = memory.store_analysis(profile.get('filename'), analysis_summary)\n",
        "print(f\"Analysis stored in Memory Bank with ID: {memory_id}\")\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\"Step 4: Compiling Report...\")\n",
        "print(\"============================================================\")\n",
        "start_report = time.time()\n",
        "reporter_agent = ReporterAgent(session_service, logger)\n",
        "report = reporter_agent.generate_report()\n",
        "report_time = time.time() - start_report\n",
        "print(f\"Report generated ({report_time:.2f}s). File: {report.get('report_filename')}\")\n",
        "\n",
        "# shows download link for convenience\n",
        "if report.get('report_path') and os.path.exists(report.get('report_path')):\n",
        "    print(f\"\\n‚úÖ Full Pipeline Demo Complete! Report available at: {report.get('report_path')}\")\n",
        "    display(FileLink(report.get(\"report_path\")))\n",
        "else:\n",
        "    # shows files in OUTPUT_DIR to help debugging\n",
        "    print(\"\\n‚ùå Report generation failed or report not found. Listing OUTPUT_DIR for debugging:\")\n",
        "    print(os.listdir(OUTPUT_DIR))\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\"TOTAL PIPELINE DURATION: \", round(ingest_time + analysis_time + anomaly_time + report_time, 2), \"s\")\n",
        "print(\"============================================================\")\n"
      ],
      "metadata": {
        "id": "T3e2m8J1bfxN",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# üîµ ONE-CELL DEMO (robust & portable)\n",
        "from IPython.display import FileLink, display\n",
        "import time, os, sys, traceback\n",
        "\n",
        "# try to import google.colab.files only in Colab\n",
        "IN_COLAB = False\n",
        "try:\n",
        "    from google.colab import files as colab_files\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "# Determine repo_root: prefer cloned path, fallback to cwd\n",
        "repo_root_candidates = [\n",
        "    \"/content/DataSense_AI_Agent\",\n",
        "    os.getcwd()\n",
        "]\n",
        "repo_root = None\n",
        "for p in repo_root_candidates:\n",
        "    if os.path.isdir(p):\n",
        "        repo_root = p\n",
        "        break\n",
        "if repo_root is None:\n",
        "    repo_root = os.getcwd()\n",
        "\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "print(\"Using repo_root:\", repo_root)\n",
        "print(\"Python sys.path head:\", sys.path[:3])\n",
        "\n",
        "# Try imports and provide helpful errors if missing\n",
        "missing = []\n",
        "try:\n",
        "    from services.session_service import SessionService\n",
        "except Exception as e:\n",
        "    missing.append(\"services.session_service\")\n",
        "\n",
        "try:\n",
        "    from utils.logger import AgentLogger\n",
        "except Exception as e:\n",
        "    missing.append(\"utils.logger\")\n",
        "\n",
        "try:\n",
        "    from services.memory_bank import MemoryBank\n",
        "except Exception as e:\n",
        "    missing.append(\"services.memory_bank\")\n",
        "\n",
        "try:\n",
        "    from agents.ingest_agent import IngestAgent\n",
        "except Exception as e:\n",
        "    missing.append(\"agents.ingest_agent\")\n",
        "\n",
        "try:\n",
        "    from agents.analysis_agent import AnalysisAgent\n",
        "except Exception as e:\n",
        "    missing.append(\"agents.analysis_agent\")\n",
        "\n",
        "try:\n",
        "    from agents.anomaly_agent import AnomalyAgent\n",
        "except Exception as e:\n",
        "    missing.append(\"agents.anomaly_agent\")\n",
        "\n",
        "try:\n",
        "    from agents.reporter_agent import ReporterAgent\n",
        "except Exception as e:\n",
        "    missing.append(\"agents.reporter_agent\")\n",
        "\n",
        "try:\n",
        "    from config import Config\n",
        "except Exception as e:\n",
        "    missing.append(\"config (Config)\")\n",
        "\n",
        "if missing:\n",
        "    print(\"ERROR: The following modules are missing or failed to import:\")\n",
        "    for m in missing:\n",
        "        print(\" -\", m)\n",
        "    print(\"\\nMake sure the repository is in repo_root and those files exist. Aborting demo.\")\n",
        "    raise ImportError(\"Missing modules: \" + \", \".join(missing))\n",
        "\n",
        "# Prepare OUTPUT_DIR\n",
        "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"/tmp/datasense_outputs\")\n",
        "OUTPUT_DIR = os.path.abspath(OUTPUT_DIR)\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
        "\n",
        "print(\"\\nüöÄ Starting ONE-CELL FULL PIPELINE DEMO (Upload or fallback to sample)\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(getattr(Config, \"LOGS_PATH\", OUTPUT_DIR))\n",
        "session_service = SessionService()\n",
        "memory = MemoryBank()\n",
        "\n",
        "print(f\"Session created: {getattr(session_service, 'session_id', 'unknown-session')}\")\n",
        "print(\"Memory Bank initialized.\\n\")\n",
        "\n",
        "# --- Upload prompt (Colab only) ---\n",
        "sample_file_path = None\n",
        "if IN_COLAB:\n",
        "    print(\"üîΩ Upload a CSV file now (or cancel to use bundled sample):\")\n",
        "    uploaded = colab_files.upload()\n",
        "    if uploaded:\n",
        "        uploaded_name = list(uploaded.keys())[0]\n",
        "        saved_path = os.path.join(OUTPUT_DIR, uploaded_name)\n",
        "        with open(saved_path, \"wb\") as f:\n",
        "            f.write(uploaded[uploaded_name])\n",
        "        sample_file_path = saved_path\n",
        "        print(f\"Uploaded file saved to: {sample_file_path}\")\n",
        "    else:\n",
        "        print(\"No file uploaded in Colab upload dialog.\")\n",
        "else:\n",
        "    print(\"Not running in Colab ‚Äî skipping interactive upload. Will use bundled sample if available.\")\n",
        "\n",
        "# Fallback to sample in repo\n",
        "if not sample_file_path:\n",
        "    sample_file_path = os.path.join(repo_root, getattr(Config, \"BASE_PATH\", \"sample_data\"), \"sales.csv\")\n",
        "    print(\"Using fallback sample path:\", sample_file_path)\n",
        "\n",
        "# Validate dataset path\n",
        "if not os.path.exists(sample_file_path):\n",
        "    print(\"\\nERROR: Dataset path not found:\", sample_file_path)\n",
        "    print(\"Please upload a CSV or ensure sample_data/sales.csv exists in repo.\")\n",
        "    raise FileNotFoundError(sample_file_path)\n",
        "\n",
        "# --- Pipeline Steps with robust error handling ---\n",
        "def safe_call(step_name, fn, *args, **kwargs):\n",
        "    print(f\"\\n== {step_name} ==\")\n",
        "    start = time.time()\n",
        "    try:\n",
        "        res = fn(*args, **kwargs)\n",
        "        elapsed = time.time() - start\n",
        "        print(f\"{step_name} completed ({elapsed:.2f}s).\")\n",
        "        return res, elapsed\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR during {step_name}: {e}\")\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "# Step 1: Ingest\n",
        "ingest_agent = IngestAgent(session_service, logger)\n",
        "try:\n",
        "    profile = None\n",
        "    if hasattr(ingest_agent, \"ingest_csv\"):\n",
        "        profile, t_ing = safe_call(\"Ingest Agent (ingest_csv)\", ingest_agent.ingest_csv, sample_file_path)\n",
        "        # some implementations return just profile, not (profile, time)\n",
        "        if isinstance(profile, tuple) and len(profile) == 2 and isinstance(profile[1], (int,float)):\n",
        "            profile = profile[0]\n",
        "    else:\n",
        "        # fallback to reading DataFrame and calling ingest_dataframe / storing DF\n",
        "        import pandas as pd\n",
        "        df_local = pd.read_csv(sample_file_path)\n",
        "        if hasattr(ingest_agent, \"ingest_dataframe\"):\n",
        "            profile, t_ing = safe_call(\"Ingest Agent (ingest_dataframe)\", ingest_agent.ingest_dataframe, df_local)\n",
        "        else:\n",
        "            session_service.store(getattr(session_service,'session_id'), \"df\", df_local)\n",
        "            profile = {\"filename\": os.path.basename(sample_file_path), \"quality_metrics\": {\"completeness_score\": None, \"overall_quality\": None}}\n",
        "            t_ing = 0.0\n",
        "            print(\"Warning: ingest_agent has no ingest_csv/ingest_dataframe; stored DataFrame in session.\")\n",
        "except Exception as e:\n",
        "    raise RuntimeError(\"Ingest failed; fix ingest_agent or dataset.\") from e\n",
        "\n",
        "# Get DataFrame\n",
        "if hasattr(ingest_agent, \"get_dataframe\"):\n",
        "    df = ingest_agent.get_dataframe()\n",
        "else:\n",
        "    df = session_service.get(getattr(session_service,'session_id'), \"df\")\n",
        "if df is None:\n",
        "    raise RuntimeError(\"Could not obtain DataFrame from ingest step. Check ingest_agent implementation.\")\n",
        "\n",
        "print(f\"Ingestion result: file={profile.get('filename')}, rows={getattr(df, 'shape', ('?', '?'))[0]}\")\n",
        "\n",
        "# Step 2: Analysis\n",
        "analysis_agent_instance = AnalysisAgent(session_service, logger)\n",
        "analysis_res, t_analysis = safe_call(\"Analysis Agent (analyze)\", analysis_agent_instance.analyze, df)\n",
        "insights = analysis_res.get('insights', [])\n",
        "visuals = analysis_res.get('visualizations', [])\n",
        "\n",
        "print(f\"Insights count: {len(insights)}, Visualizations count: {len(visuals)}\")\n",
        "\n",
        "# Step 3: Anomaly detection\n",
        "anomaly_agent_instance = AnomalyAgent(session_service, logger)\n",
        "anomaly_res, t_anom = safe_call(\"Anomaly Agent (detect_anomalies)\", anomaly_agent_instance.detect_anomalies, df)\n",
        "total_anomalies = anomaly_res.get('total_anomalies', 0)\n",
        "print(f\"Detected anomalies: {total_anomalies}\")\n",
        "\n",
        "# Step 4: Store in Memory\n",
        "analysis_summary = {\n",
        "    'shape': getattr(df, \"shape\", None),\n",
        "    'columns': list(getattr(df, \"columns\", [])),\n",
        "    'anomalies_count': total_anomalies,\n",
        "    'quality_score': profile.get('quality_metrics', {}).get('completeness_score'),\n",
        "    'key_findings': [\n",
        "        f\"Detected {total_anomalies} anomalies\",\n",
        "        f\"Quality: {profile.get('quality_metrics', {}).get('overall_quality')}\",\n",
        "        f\"Generated {len(visuals)} visualizations\"\n",
        "    ]\n",
        "}\n",
        "if hasattr(memory, \"store_analysis\"):\n",
        "    memory_id = memory.store_analysis(profile.get('filename'), analysis_summary)\n",
        "    print(f\"Analysis stored in Memory Bank with ID: {memory_id}\")\n",
        "else:\n",
        "    print(\"MemoryBank.store_analysis not found. Skipping memory storage.\")\n",
        "    memory_id = None\n",
        "\n",
        "# Step 5: Report\n",
        "reporter_agent = ReporterAgent(session_service, logger)\n",
        "report, t_report = None, 0.0\n",
        "if hasattr(reporter_agent, \"generate_report\"):\n",
        "    report, t_report = safe_call(\"Reporter Agent (generate_report)\", reporter_agent.generate_report)\n",
        "else:\n",
        "    print(\"ReporterAgent.generate_report not implemented. Skipping report generation.\")\n",
        "\n",
        "# Show report link if available\n",
        "if isinstance(report, dict):\n",
        "    report_path = report.get('report_path') or report.get('report_filename')\n",
        "    if report_path and os.path.exists(report_path):\n",
        "        print(f\"\\n‚úÖ Full Pipeline Demo Complete! Report available at: {report_path}\")\n",
        "        display(FileLink(report_path))\n",
        "    else:\n",
        "        # try OUTPUT_DIR listing\n",
        "        print(\"\\n‚ùå Report path not found in report dict or file missing. OUTPUT_DIR listing for debugging:\")\n",
        "        print(os.listdir(OUTPUT_DIR))\n",
        "else:\n",
        "    print(\"\\n‚ùå Reporter did not return a report dict. Reporter output:\", report)\n",
        "\n",
        "total_time = round((t_ing if 't_ing' in locals() else 0) + (t_analysis if 't_analysis' in locals() else 0) + (t_anom if 't_anom' in locals() else 0) + (t_report if 't_report' in locals() else 0), 2)\n",
        "print(\"\\n============================================================\")\n",
        "print(\"TOTAL PIPELINE DURATION: \", total_time, \"s\")\n",
        "print(\"============================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "O9mv0Pw2oKvT",
        "outputId": "4da8801f-d54d-4dcc-aeb1-7df097738dc3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using repo_root: /content\n",
            "Python sys.path head: ['/content/drive/MyDrive/DataSense_AI_Agent', '/content/DataSense_AI_Agent', '/content']\n",
            "OUTPUT_DIR: /tmp/datasense_outputs\n",
            "\n",
            "üöÄ Starting ONE-CELL FULL PIPELINE DEMO (Upload or fallback to sample)\n",
            "\n",
            "üìå Session 20251203_150940 created\n",
            "üìö Memory Bank initialized: /content/drive/MyDrive/DataSense_AI_Agent/outputs/memory\n",
            "Session created: 20251203_150940\n",
            "Memory Bank initialized.\n",
            "\n",
            "üîΩ Upload a CSV file now (or cancel to use bundled sample):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ab4d3849-6136-4e39-82bb-2a62c5155c3b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ab4d3849-6136-4e39-82bb-2a62c5155c3b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:DataSenseAI:LLM enhancement failed: \n",
            "  No API_KEY or ADC found. Please either:\n",
            "    - Set the `GOOGLE_API_KEY` environment variable.\n",
            "    - Manually pass the key with `genai.configure(api_key=my_api_key)`.\n",
            "    - Or set up Application Default Credentials, see https://ai.google.dev/gemini-api/docs/oauth for more information.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving synthetic_dataset.csv to synthetic_dataset.csv\n",
            "Uploaded file saved to: /tmp/datasense_outputs/synthetic_dataset.csv\n",
            "ü§ñ IngestAgent state updated\n",
            "\n",
            "== Ingest Agent (ingest_csv) ==\n",
            "ü§ñ IngestAgent state updated\n",
            "\n",
            "============================================================\n",
            "ü§ñ INGEST AGENT STARTING\n",
            "============================================================\n",
            "üìÇ Loading: /tmp/datasense_outputs/synthetic_dataset.csv\n",
            "‚úì Loaded 4,362 rows √ó 5 columns\n",
            "\n",
            "üìä Creating dataset profile...\n",
            "  ‚Ä¢ Analyzing structure...\n",
            "  ‚Ä¢ Analyzing missing values...\n",
            "  ‚Ä¢ Analyzing numeric columns...\n",
            "  ‚Ä¢ Analyzing categorical columns...\n",
            "  ‚Ä¢ Detecting date columns...\n",
            "  ‚Ä¢ Creating sample data...\n",
            "  ‚Ä¢ Computing quality metrics...\n",
            "üìä Dataset info stored: synthetic_dataset.csv\n",
            "‚úì Profile created and stored in session\n",
            "\n",
            "ü§ñ Getting AI insights...\n",
            "ü§ñ IngestAgent state updated\n",
            "‚úì Completed in 0.02s\n",
            "============================================================\n",
            "\n",
            "Ingest Agent (ingest_csv) completed (0.02s).\n",
            "Ingestion result: file=synthetic_dataset.csv, rows=4362\n",
            "ü§ñ AnalysisAgent state updated\n",
            "\n",
            "== Analysis Agent (analyze) ==\n",
            "ü§ñ AnalysisAgent state updated\n",
            "ü§ñ AnalysisAgent state updated\n",
            "\n",
            "============================================================\n",
            "ü§ñ ANALYSIS AGENT STARTING\n",
            "============================================================\n",
            "üìä Dataset has 3 numeric, 2 categorical\n",
            "\n",
            "1Ô∏è‚É£  Analyzing data descriptions...\n",
            "  ‚úì Description analysis complete\n",
            "\n",
            "2Ô∏è‚É£  Analyzing correlations...\n",
            "  ‚úì Correlation analysis: 3 top pairs found\n",
            "\n",
            "3Ô∏è‚É£  Creating distribution plots...\n",
            "  ‚úì Created 3 distribution plots\n",
            "\n",
            "4Ô∏è‚É£  Analyzing missing values...\n",
            "  ‚úì Missing value analysis complete\n",
            "\n",
            "5Ô∏è‚É£  Analyzing categorical variables...\n",
            "  ‚úì Categorical analysis for 2 columns\n",
            "\n",
            "üí° Generating insights...\n",
            "üí° Insight added: overview\n",
            "üí° Insight added: data_quality\n",
            "üí° Insight added: correlation\n",
            "üí° Insight added: resource\n",
            "üìä Plot registered: distribution_histograms\n",
            "ü§ñ AnalysisAgent state updated\n",
            "‚úì Analysis complete in 1.12s\n",
            "============================================================\n",
            "\n",
            "Analysis Agent (analyze) completed (1.12s).\n",
            "Insights count: 4, Visualizations count: 1\n",
            "ü§ñ AnomalyAgent state updated\n",
            "\n",
            "== Anomaly Agent (detect_anomalies) ==\n",
            "ü§ñ AnomalyAgent state updated\n",
            "\n",
            "============================================================\n",
            "ü§ñ ANOMALY DETECTION AGENT STARTING\n",
            "============================================================\n",
            "\n",
            "‚ö° Running anomaly detection methods in PARALLEL...\n",
            "  ‚Üí Running Z-Score detection...\n",
            "  ‚Üí Running IQR detection...\n",
            "  ‚Üí Running duplicate detection...\n",
            "  ‚úì Z-Score: 0 found\n",
            "  ‚úì IQR: 0 found\n",
            "  ‚úì Duplicates: 25 found\n",
            "\n",
            "üìä Consolidating results...\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "‚ö†Ô∏è  Anomaly added (low): duplicate_row\n",
            "ü§ñ AnomalyAgent state updated\n",
            "\n",
            "‚úì Anomaly detection complete in 0.03s\n",
            "============================================================\n",
            "\n",
            "Anomaly Agent (detect_anomalies) completed (0.03s).\n",
            "Detected anomalies: 25\n",
            "üíæ Stored memory: synthetic_dataset.csv_20251203_150953\n",
            "Analysis stored in Memory Bank with ID: synthetic_dataset.csv_20251203_150953\n",
            "ü§ñ ReporterAgent state updated\n",
            "\n",
            "== Reporter Agent (generate_report) ==\n",
            "ü§ñ ReporterAgent state updated\n",
            "\n",
            "============================================================\n",
            "ü§ñ REPORTER AGENT STARTING\n",
            "============================================================\n",
            "\n",
            "üìÑ Building report sections...\n",
            "  -> Cover page\n",
            "  -> Executive summary\n",
            "  -> Dataset overview\n",
            "  -> Analysis results\n",
            "  -> Anomalies detected\n",
            "  -> Insights\n",
            "  -> Technical details\n",
            "\n",
            "üíæ Saving PDF...\n",
            "‚úì Report saved: Report_synthetic_dataset_20251203_150955.pdf\n",
            "ü§ñ ReporterAgent state updated\n",
            "Reporter Agent (generate_report) completed (1.61s).\n",
            "\n",
            "‚úÖ Full Pipeline Demo Complete! Report available at: /content/drive/MyDrive/DataSense_AI_Agent/outputs/reports/Report_synthetic_dataset_20251203_150955.pdf\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/content/drive/MyDrive/DataSense_AI_Agent/outputs/reports/Report_synthetic_dataset_20251203_150955.pdf"
            ],
            "text/html": [
              "<a href='/content/drive/MyDrive/DataSense_AI_Agent/outputs/reports/Report_synthetic_dataset_20251203_150955.pdf' target='_blank'>/content/drive/MyDrive/DataSense_AI_Agent/outputs/reports/Report_synthetic_dataset_20251203_150955.pdf</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "TOTAL PIPELINE DURATION:  2.78 s\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "83W0DWvr8QBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Setup API Key (SECURE)\n",
        "from google.colab import userdata\n",
        "\n",
        "# Now code can use it:\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "print(\"‚úÖ API Key loaded securely!\")"
      ],
      "metadata": {
        "id": "-b_eGkqQ9k15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Install Required Packages\n",
        "\n",
        "!pip install -q google-generativeai  # Google's Gemini AI\n",
        "!pip install -q langgraph            # For multi-agent orchestration\n",
        "!pip install -q pandas               # For data manipulation\n",
        "!pip install -q matplotlib           # For creating charts\n",
        "!pip install -q seaborn              # For beautiful charts\n",
        "!pip install -q fpdf                 # For creating PDF reports\n",
        "!pip install -q scikit-learn         # For data analysis tools\n",
        "!pip install -q scipy                # For scientific computing\n",
        "\n",
        "print(\"‚úÖ All packages installed!\")\n"
      ],
      "metadata": {
        "id": "nj5uWP2G-RbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Initialize Gemini\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key from secrets\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Configure the AI\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Test it works\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "response = model.generate_content(\"Say 'Hello! I'm ready to help analyze data!'\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "m1lhBTtJIm9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create Sample Sales Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "from config import Config\n",
        "\n",
        "# Set random seed for reproducibility (same data every time)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create date range (Jan 1 to Nov 25, 2024)\n",
        "dates = pd.date_range(start='2024-01-01', end='2024-11-25', freq='D')\n",
        "num_days = len(dates)\n",
        "\n",
        "print(f\"Creating {num_days} days of sales data...\")\n",
        "\n",
        "# Create the dataset\n",
        "sales_data = pd.DataFrame({\n",
        "    # Date column\n",
        "    'date': dates,\n",
        "\n",
        "    # Product IDs (5 different products)\n",
        "    'product_id': np.random.choice(\n",
        "        ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
        "        num_days\n",
        "    ),\n",
        "\n",
        "    # Sales amount (normally distributed around $5000)\n",
        "    # np.random.normal(mean, std_deviation, count)\n",
        "    'sales_amount': np.random.normal(5000, 1500, num_days),\n",
        "\n",
        "    # Quantity sold (Poisson distribution, average 50 units)\n",
        "    'quantity_sold': np.random.poisson(50, num_days),\n",
        "\n",
        "    # Region (4 regions)\n",
        "    'region': np.random.choice(\n",
        "        ['North', 'South', 'East', 'West'],\n",
        "        num_days\n",
        "    ),\n",
        "\n",
        "    # Customer segment\n",
        "    'customer_segment': np.random.choice(\n",
        "        ['Retail', 'Wholesale', 'Online'],\n",
        "        num_days\n",
        "    )\n",
        "})\n",
        "\n",
        "# ‚ñÑ Add realistic anomalies (this is what we'll detect!)\n",
        "print(\"Adding anomalies...\")\n",
        "\n",
        "# Pick 15 random days to be anomalies\n",
        "anomaly_indices = np.random.choice(num_days, 15, replace=False)\n",
        "\n",
        "# Create spike anomalies (sudden sales increase)\n",
        "spike_indices = anomaly_indices[:10]\n",
        "sales_data.loc[spike_indices, 'sales_amount'] *= 3\n",
        "print(f\"  ‚ÜóÔ∏è  Added {len(spike_indices)} spike anomalies (3x normal sales)\")\n",
        "\n",
        "# Create drop anomalies (sudden sales decrease)\n",
        "drop_indices = anomaly_indices[10:]\n",
        "sales_data.loc[drop_indices, 'sales_amount'] *= 0.1\n",
        "print(f\"  ‚ÜòÔ∏è  Added {len(drop_indices)} drop anomalies (10% normal sales)\")\n",
        "\n",
        "# Add missing values (realistic data quality issues)\n",
        "missing_indices = np.random.choice(num_days, 10, replace=False)\n",
        "sales_data.loc[missing_indices, 'customer_segment'] = np.nan\n",
        "print(f\"  ‚ùì Added {len(missing_indices)} missing values\")\n",
        "\n",
        "# Save to Google Drive\n",
        "output_path = f'{Config.BASE_PATH}/sample_data/sales.csv'\n",
        "sales_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Created: sales.csv\")\n",
        "print(f\"   Shape: {sales_data.shape[0]} rows √ó {sales_data.shape[1]} columns\")\n",
        "print(f\"   Saved to: {output_path}\")\n",
        "\n",
        "# Show preview\n",
        "print(\"\\nüìä Preview (first 5 rows):\")\n",
        "print(sales_data.head())\n"
      ],
      "metadata": {
        "id": "hg6jbg9zSuB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Create Transaction Dataset\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "from config import Config\n",
        "\n",
        "print(\"Creating transaction dataset...\")\n",
        "\n",
        "transactions = pd.DataFrame({\n",
        "    'transaction_id': [f'T{i:05d}' for i in range(1, 1001)],  # T00001, T00002, ...\n",
        "    'timestamp': pd.date_range(start='2024-01-01', periods=1000, freq='h'),\n",
        "    'user_id': np.random.randint(1000, 5000, 1000),\n",
        "    'amount': np.random.exponential(100, 1000),  # Most small, few large\n",
        "    'payment_method': np.random.choice(\n",
        "        ['Credit', 'Debit', 'UPI', 'Cash'],\n",
        "        1000\n",
        "    ),\n",
        "    'status': np.random.choice(\n",
        "        ['Success', 'Failed', 'Pending'],\n",
        "        1000,\n",
        "        p=[0.9, 0.08, 0.02]  # 90% success, 8% failed, 2% pending\n",
        "    )\n",
        "})\n",
        "\n",
        "transactions.to_csv(f'{Config.BASE_PATH}/sample_data/transactions.csv', index=False)\n",
        "print(f\"‚úÖ Created: transactions.csv ({transactions.shape[0]} rows)\")"
      ],
      "metadata": {
        "id": "GpcESvHWXCID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create Customer Dataset\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "from config import Config\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Creating customer dataset...\")\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    'customer_id': range(1, 501),\n",
        "    'age': np.random.randint(18, 75, 500),\n",
        "    'income': np.random.lognormal(10.5, 0.5, 500),  # Income distribution\n",
        "    'credit_score': np.random.randint(300, 850, 500),\n",
        "    'tenure_months': np.random.randint(1, 120, 500),\n",
        "    'churn': np.random.choice([0, 1], 500, p=[0.85, 0.15])  # 15% churned\n",
        "})\n",
        "\n",
        "customers.to_csv(f'{Config.BASE_PATH}/sample_data/customer_data.csv', index=False)\n",
        "print(f\"‚úÖ Created: customer_data.csv ({customers.shape[0]} rows)\")\n",
        "\n",
        "print(\"\\nüéâ All sample datasets created!\")"
      ],
      "metadata": {
        "id": "xHZVd0a9XKxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test Config\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "from config import Config\n",
        "\n",
        "print(\"Testing configuration...\")\n",
        "print(f\"‚úì Base path: {Config.BASE_PATH}\")\n",
        "print(f\"‚úì Model: {Config.GEMINI_MODEL}\")\n",
        "print(f\"‚úì Anomaly threshold: {Config.ANOMALY_THRESHOLD}\")\n",
        "print(\"\\n‚úÖ Config works!\")\n"
      ],
      "metadata": {
        "id": "t06r-lNahDBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Test Logger\n",
        "from utils.logger import AgentLogger\n",
        "from config import Config\n",
        "\n",
        "# Create logger\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "\n",
        "# Simulate agent activity\n",
        "logger.log_agent_start('TestAgent', {'test': 'data'})\n",
        "import time\n",
        "time.sleep(1)  # Simulate work\n",
        "logger.log_agent_end('TestAgent', {'result': 'success'}, 1.0)\n",
        "\n",
        "# Simulate tool call\n",
        "logger.log_tool_call('TestTool', {'param': 'value'}, 'Tool output')\n",
        "\n",
        "# Print summary\n",
        "logger.print_summary()\n"
      ],
      "metadata": {
        "id": "29ojLB6ttar7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Test Session Service\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "import os, sys\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "\n",
        "# Create a session\n",
        "session = SessionService()\n",
        "\n",
        "# Simulate Ingest Agent storing dataset info\n",
        "session.store_dataset_info({\n",
        "    'filename': 'sales.csv',\n",
        "    'shape': (330, 6),\n",
        "    'missing_percentage': {'date': 0, 'sales_amount': 0.5}\n",
        "})\n",
        "\n",
        "# Simulate Anomaly Agent finding issues\n",
        "session.add_anomaly({\n",
        "    'type': 'spike',\n",
        "    'column': 'sales_amount',\n",
        "    'row': 42,\n",
        "    'value': 15000,\n",
        "    'severity': 'high'\n",
        "})\n",
        "\n",
        "session.add_anomaly({\n",
        "    'type': 'outlier',\n",
        "    'column': 'quantity_sold',\n",
        "    'row': 105,\n",
        "    'value': 500,\n",
        "    'severity': 'medium'\n",
        "})\n",
        "\n",
        "# Simulate Analysis Agent storing insights\n",
        "session.add_insight(\n",
        "    'North region has 2.5x higher average sales',\n",
        "    'regional_analysis'\n",
        ")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "summary = session.get_session_summary()\n",
        "for key, value in summary.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "W1YlYNS0vE26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Test Agent Communication Protocol\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "from agents import AgentRole, AgentMessage, BaseAgent\n",
        "\n",
        "# Create a test message\n",
        "message = AgentMessage(\n",
        "    sender=AgentRole.INGEST,\n",
        "    receiver=AgentRole.ANALYSIS,\n",
        "    message_type=\"request\",\n",
        "    payload={\n",
        "        'dataset_name': 'sales.csv',\n",
        "        'rows': 330,\n",
        "        'columns': ['date', 'sales', 'region']\n",
        "    },\n",
        "    timestamp=\"2024-11-27T01:23:45\",\n",
        "    session_id=\"20241127_012345\"\n",
        ")\n",
        "\n",
        "# Show the message\n",
        "print(\"Message created:\")\n",
        "print(message.to_json())\n",
        "\n",
        "# Convert back from dict\n",
        "message_dict = message.to_dict()\n",
        "reconstructed = AgentMessage.from_dict(message_dict)\n",
        "\n",
        "print(\"\\n‚úÖ Message conversion successful!\")\n",
        "print(f\"Sender: {reconstructed.sender.value}\")\n",
        "print(f\"Receiver: {reconstructed.receiver.value}\")\n",
        "print(f\"Payload: {reconstructed.payload}\")\n"
      ],
      "metadata": {
        "id": "5dhZ4wb_xPg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Test Complete Pipeline\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "from config import Config\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "\n",
        "print(\"üöÄ Starting Integration Test\\n\")\n",
        "\n",
        "# ===== Step 1: Initialize Services =====\n",
        "print(\"Step 1: Initializing services...\")\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "print(\"‚úì Logger and Session created\\n\")\n",
        "\n",
        "# ===== Step 2: Create Ingest Agent =====\n",
        "print(\"Step 2: Creating Ingest Agent...\")\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "print(\"‚úì Ingest Agent created\\n\")\n",
        "\n",
        "# ===== Step 3: Ingest Dataset =====\n",
        "print(\"Step 3: Ingesting dataset...\")\n",
        "profile = ingest_agent.ingest_csv(\n",
        "    f'{Config.BASE_PATH}/sample_data/sales.csv'\n",
        ")\n",
        "print(\"‚úì Dataset ingested\\n\")\n",
        "\n",
        "# ===== Step 4: Display Results =====\n",
        "print(\"=\"*60)\n",
        "print(\"INGEST AGENT RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Dataset: {profile['filename']}\")\n",
        "print(f\"üìè Shape: {profile['shape'][0]:,} rows √ó {profile['shape'][1]} columns\")\n",
        "print(f\"üíæ Memory: {profile['memory_usage_mb']} MB\")\n",
        "\n",
        "print(f\"\\n‚úÖ Quality: {profile['quality_metrics']['overall_quality']}\")\n",
        "print(f\"üìà Completeness: {profile['quality_metrics']['completeness_score']}%\")\n",
        "print(f\"üîÑ Duplicates: {profile['quality_metrics']['duplicate_rows']}\")\n",
        "\n",
        "print(f\"\\nüìã Columns: {', '.join(profile['columns'])}\")\n",
        "print(f\"üìÖ Date Columns: {', '.join(profile['date_columns'])}\")\n",
        "\n",
        "print(f\"\\nü§ñ AI Analysis:\")\n",
        "print(f\"   Type: {profile['llm_insights']['dataset_type']}\")\n",
        "print(f\"   Recommended: {', '.join(profile['llm_insights']['recommended_analysis'][:2])}\")\n",
        "\n",
        "if profile['llm_insights']['potential_issues']:\n",
        "    print(f\"   Issues: {', '.join(profile['llm_insights']['potential_issues'][:2])}\")\n",
        "\n",
        "# ===== Step 5: Show Session State =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION STATE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary = session.get_session_summary()\n",
        "print(f\"\\n Session ID: {summary['session_id']}\")\n",
        "print(f\"   Duration: {summary['duration_seconds']:.2f}s\")\n",
        "print(f\"   Dataset: {summary['dataset_name']}\")\n",
        "print(f\"   Agents: {', '.join(summary['agents_executed'])}\")\n",
        "\n",
        "# ===== Step 6: Show Metrics =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXECUTION METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics = logger.get_metrics_summary()\n",
        "logger.print_summary()\n",
        "\n",
        "print(\"\\n‚úÖ Integration Test Complete!\")\n"
      ],
      "metadata": {
        "id": "E10NUcSJ0sSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Test Code Executor\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils.logger import AgentLogger\n",
        "from tools.code_executor import CodeExecutor\n",
        "from config import Config\n",
        "\n",
        "# Create logger\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "\n",
        "# Create executor\n",
        "executor = CodeExecutor(logger)\n",
        "\n",
        "# Create sample dataframe\n",
        "df = pd.DataFrame({\n",
        "    'Product': ['A', 'B', 'C', 'A', 'B'],\n",
        "    'Sales': [1000, 1500, 2000, 1200, 1800],\n",
        "    'Region': ['North', 'South', 'East', 'North', 'South']\n",
        "})\n",
        "\n",
        "# Test 1: Simple calculation\n",
        "print(\"=\" * 60)\n",
        "print(\"TEST 1: Simple Calculation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "code1 = \"\"\"\n",
        "print(\"Total Sales:\", df['Sales'].sum())\n",
        "print(\"Average Sales:\", df['Sales'].mean())\n",
        "print(\"Max Sales:\", df['Sales'].max())\n",
        "\n",
        "result = df.groupby('Region')['Sales'].sum()\n",
        "\"\"\"\n",
        "\n",
        "output1 = executor.run_code(code1, extra_context={'df': df})\n",
        "\n",
        "print(f\"‚úÖ Success: {output1['success']}\")\n",
        "print(f\"Output:\\n{output1['stdout']}\")\n",
        "if output1['result'] is not None:\n",
        "    print(f\"Result:\\n{output1['result']}\")\n",
        "\n",
        "# Test 2: Create visualization\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEST 2: Create Visualization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "code2 = \"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df['Product'], df['Sales'], color='steelblue')\n",
        "plt.title('Sales by Product')\n",
        "plt.xlabel('Product')\n",
        "plt.ylabel('Sales')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\"\"\"\n",
        "\n",
        "output2 = executor.run_code(\n",
        "    code2,\n",
        "    extra_context={'df': df},\n",
        "    plot_filename='test_bar_chart.png'\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Success: {output2['success']}\")\n",
        "print(f\"Plot saved at: {output2['plot_path']}\")\n",
        "\n",
        "# Test 3: Error handling\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEST 3: Error Handling\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "code3 = \"\"\"\n",
        "# This will cause an error\n",
        "result = df['NonExistentColumn'].sum()\n",
        "\"\"\"\n",
        "\n",
        "output3 = executor.run_code(code3, extra_context={'df': df})\n",
        "\n",
        "print(f\"‚ùå Success: {output3['success']}\")\n",
        "print(f\"Error:\\n{output3['error']}\")\n",
        "\n",
        "print(\"\\n‚úÖ Code Executor tests complete!\")\n"
      ],
      "metadata": {
        "id": "IBFSx9pp8J5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Test Analysis Agent\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Testing Analysis Agent\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "\n",
        "# Load data with Ingest Agent\n",
        "print(\"Step 1: Loading data...\")\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "print(\"\\nStep 2: Running analysis...\")\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Dataset: {analysis_results['dataset_shape']}\")\n",
        "print(f\"üìà Numeric columns: {', '.join(analysis_results['numeric_columns'][:3])}\")\n",
        "print(f\"üìù Categorical columns: {', '.join(analysis_results['categorical_columns'][:2])}\")\n",
        "\n",
        "print(f\"\\nüìä Visualizations created: {len(analysis_results['visualizations'])}\")\n",
        "for viz in analysis_results['visualizations']:\n",
        "    print(f\"  ‚Ä¢ {viz['type']}: {viz['description']}\")\n",
        "\n",
        "print(f\"\\nüí° Insights: {len(analysis_results['insights'])}\")\n",
        "for insight in analysis_results['insights']:\n",
        "    print(f\"  ‚Ä¢ [{insight['severity']}] {insight['text']}\")\n",
        "\n",
        "# Show session state\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION STATE\")\n",
        "print(\"=\"*60)\n",
        "summary = session.get_session_summary()\n",
        "print(f\"\\nTotal insights: {summary['total_insights']}\")\n",
        "print(f\"Total plots: {summary['total_plots']}\")\n",
        "print(f\"Agents executed: {', '.join(summary['agents_executed'])}\")\n",
        "\n",
        "print(\"\\n‚úÖ Analysis Agent test complete!\")\n"
      ],
      "metadata": {
        "id": "_AcO0BHK_kdg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Integration Test\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from tools.alert_tool import AlertTool, AlertSeverity\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Integration Test\\n\")\n",
        "\n",
        "# ===== Initialize all services =====\n",
        "print(\"Initializing services...\")\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "alert_tool = AlertTool(logger)\n",
        "\n",
        "# ===== Step 1: Ingest Data =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: Data Ingestion\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"Data Loaded Successfully\",\n",
        "    f\"Loaded {df.shape[0]:,} rows √ó {df.shape[1]} columns\",\n",
        "    AlertSeverity.INFO,\n",
        "    {'dataset': 'sales.csv', 'shape': df.shape}\n",
        ")\n",
        "\n",
        "# ===== Step 2: Analyze Data =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: Data Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# Alert if quality issues found\n",
        "quality = profile['quality_metrics']\n",
        "if quality['overall_quality'] != 'Good':\n",
        "    alert_tool.send_alert(\n",
        "        \"Data Quality Alert\",\n",
        "        f\"Quality level: {quality['overall_quality']}\",\n",
        "        AlertSeverity.WARNING\n",
        "    )\n",
        "\n",
        "# ===== Step 3: Generate Report =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary = session.get_session_summary()\n",
        "\n",
        "print(f\"\\nüìä Session Summary:\")\n",
        "print(f\"   ID: {summary['session_id']}\")\n",
        "print(f\"   Duration: {summary['duration_seconds']:.2f}s\")\n",
        "print(f\"   Dataset: {summary['dataset_name']} ({summary['dataset_shape']})\")\n",
        "\n",
        "print(f\"\\nüìà Analysis Performed:\")\n",
        "print(f\"   Anomalies detected: {summary['total_anomalies']}\")\n",
        "print(f\"   Plots generated: {summary['total_plots']}\")\n",
        "print(f\"   Insights found: {summary['total_insights']}\")\n",
        "\n",
        "print(f\"\\nü§ñ Agents Executed:\")\n",
        "for agent in summary['agents_executed']:\n",
        "    print(f\"   ‚Ä¢ {agent}\")\n",
        "\n",
        "print(f\"\\nüîî Alerts Sent: {len(alert_tool.get_all_alerts())}\")\n",
        "for alert in alert_tool.get_all_alerts():\n",
        "    print(f\"   ‚Ä¢ [{alert['severity']}] {alert['title']}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úÖ Day 2 Integration Complete!\")\n",
        "print(f\"{'='*60}\")\n"
      ],
      "metadata": {
        "id": "QJUekvYXoKlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: Test Parallel Anomaly Detection\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Testing Parallel Anomaly Detection\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "\n",
        "# Load data\n",
        "print(\"Step 1: Loading data...\")\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "# Run analysis\n",
        "print(\"\\nStep 2: Running analysis...\")\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# Run anomaly detection (PARALLEL)\n",
        "print(\"\\nStep 3: Detecting anomalies (PARALLEL execution)...\")\n",
        "anomaly_agent = AnomalyAgent(session, logger)\n",
        "anomaly_results = anomaly_agent.detect_anomalies(df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANOMALY DETECTION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüéØ Total Anomalies: {anomaly_results['total_anomalies']}\")\n",
        "\n",
        "print(f\"\\nüìä By Detection Method:\")\n",
        "for method, count in anomaly_results['anomalies_by_method'].items():\n",
        "    print(f\"   {method}: {count}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  By Severity:\")\n",
        "for severity, count in anomaly_results['severity_breakdown'].items():\n",
        "    if count > 0:\n",
        "        print(f\"   {severity.upper()}: {count}\")\n",
        "\n",
        "print(f\"\\nüìã Top 5 Anomalies:\")\n",
        "for i, anomaly in enumerate(anomaly_results['anomalies'][:5], 1):\n",
        "    print(f\"   {i}. [{anomaly['severity']}] {anomaly['type']} - {anomaly.get('description', 'N/A')}\")\n",
        "\n",
        "# Show session state\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION STATE\")\n",
        "print(\"=\"*60)\n",
        "summary = session.get_session_summary()\n",
        "print(f\"\\nTotal anomalies stored: {summary['total_anomalies']}\")\n",
        "print(f\"Agents executed: {', '.join(summary['agents_executed'])}\")\n",
        "\n",
        "print(\"\\n‚úÖ Parallel anomaly detection test complete!\")\n"
      ],
      "metadata": {
        "id": "d6IUcNxUwzG1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Test Memory Bank\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "from services.memory_bank import MemoryBank\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Testing Memory Bank\\n\")\n",
        "\n",
        "# Create memory bank\n",
        "memory = MemoryBank()\n",
        "\n",
        "# Store an analysis\n",
        "analysis_1 = {\n",
        "    'shape': (330, 6),\n",
        "    'columns': ['date', 'product_id', 'sales_amount', 'quantity_sold', 'region'],\n",
        "    'anomalies_count': 25,\n",
        "    'quality_score': 96.97,\n",
        "    'key_findings': [\n",
        "        'Strong correlation between sales and quantity',\n",
        "        'North region has higher sales',\n",
        "        '25 anomalies detected'\n",
        "    ]\n",
        "}\n",
        "\n",
        "memory_id_1 = memory.store_analysis('sales.csv', analysis_1)\n",
        "print(f\"‚úì Stored analysis: {memory_id_1}\")\n",
        "\n",
        "# Simulate a second analysis\n",
        "print(\"\\nWaiting 2 seconds...\")\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "analysis_2 = {\n",
        "    'shape': (330, 6),\n",
        "    'columns': ['date', 'product_id', 'sales_amount', 'quantity_sold', 'region'],\n",
        "    'anomalies_count': 18,\n",
        "    'quality_score': 97.5,\n",
        "    'key_findings': [\n",
        "        'Similar patterns to previous analysis',\n",
        "        'Fewer anomalies this time',\n",
        "        'Improved data quality'\n",
        "    ]\n",
        "}\n",
        "\n",
        "memory_id_2 = memory.store_analysis('sales.csv', analysis_2)\n",
        "print(f\"‚úì Stored analysis: {memory_id_2}\")\n",
        "\n",
        "# Find similar analyses\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MEMORY BANK SEARCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "similar = memory.find_similar_analyses('sales.csv', max_results=5)\n",
        "print(f\"\\nFound {len(similar)} previous analyses of sales.csv\")\n",
        "for i, mem in enumerate(similar, 1):\n",
        "    print(f\"  {i}. {mem['memory_id']} ({mem['created_at']})\")\n",
        "\n",
        "# Compare\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison = memory.get_comparison_summary(analysis_2, 'sales.csv')\n",
        "print(f\"\\nCurrent anomalies: {comparison['current_anomalies']}\")\n",
        "print(f\"Previous analyses: {comparison['previous_analyses']}\")\n",
        "\n",
        "for comp in comparison['comparisons']:\n",
        "    print(f\"\\nPrevious ({comp['date']}):\")\n",
        "    print(f\"  Anomalies: {comp['previous_anomalies']}\")\n",
        "    print(f\"  Change: {comp['change']:+d} ({comp['trend']})\")\n",
        "\n",
        "print(\"\\n‚úÖ Memory Bank test complete!\")\n"
      ],
      "metadata": {
        "id": "Th9wcXVByxBM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: Integration Test\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from services.memory_bank import MemoryBank\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from tools.alert_tool import AlertTool, AlertSeverity\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Integration Test\\n\")\n",
        "\n",
        "# Initialize all services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "memory = MemoryBank()\n",
        "alert_tool = AlertTool(logger)\n",
        "\n",
        "# ===== PHASE 1: Data Ingestion =====\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 1: Data Ingestion\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"Dataset Loaded\",\n",
        "    f\"{df.shape[0]} rows √ó {df.shape[1]} columns\",\n",
        "    AlertSeverity.INFO\n",
        ")\n",
        "\n",
        "# ===== PHASE 2: Analysis =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: Data Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# ===== PHASE 3: Anomaly Detection (PARALLEL) =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 3: Anomaly Detection (PARALLEL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "anomaly_agent = AnomalyAgent(session, logger)\n",
        "anomaly_results = anomaly_agent.detect_anomalies(df)\n",
        "\n",
        "# Send alerts for high-severity anomalies\n",
        "high_severity = [a for a in anomaly_results['anomalies']\n",
        "                 if a.get('severity') in ['critical', 'high']]\n",
        "\n",
        "if high_severity:\n",
        "    alert_tool.send_alert(\n",
        "        f\"High-Severity Anomalies Detected\",\n",
        "        f\"Found {len(high_severity)} critical/high anomalies\",\n",
        "        AlertSeverity.WARNING,\n",
        "        {'count': len(high_severity), 'sample': high_severity[0]}\n",
        "    )\n",
        "\n",
        "# ===== PHASE 4: Memory Storage =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 4: Storing Analysis in Memory\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "analysis_summary = {\n",
        "    'shape': df.shape,\n",
        "    'columns': df.columns.tolist(),\n",
        "    'anomalies_count': len(anomaly_results['anomalies']),\n",
        "    'quality_score': profile['quality_metrics']['completeness_score'],\n",
        "    'key_findings': [\n",
        "        f\"Detected {len(anomaly_results['anomalies'])} anomalies\",\n",
        "        f\"Quality: {profile['quality_metrics']['overall_quality']}\",\n",
        "        f\"Generated {len(analysis_results['visualizations'])} visualizations\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "memory_id = memory.store_analysis('sales.csv', analysis_summary)\n",
        "print(f\"‚úì Analysis stored in memory: {memory_id}\")\n",
        "\n",
        "# ===== FINAL REPORT =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL REPORT - DAY 3 COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary = session.get_session_summary()\n",
        "\n",
        "print(f\"\\nüìä Session: {summary['session_id']}\")\n",
        "print(f\"   Duration: {summary['duration_seconds']:.2f}s\")\n",
        "print(f\"   Dataset: {summary['dataset_name']} {summary['dataset_shape']}\")\n",
        "\n",
        "print(f\"\\nüìà Analysis Results:\")\n",
        "print(f\"   Insights: {summary['total_insights']}\")\n",
        "print(f\"   Plots: {summary['total_plots']}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Anomaly Detection:\")\n",
        "print(f\"   Total: {anomaly_results['total_anomalies']}\")\n",
        "for severity, count in anomaly_results['severity_breakdown'].items():\n",
        "    if count > 0:\n",
        "        print(f\"   {severity.upper()}: {count}\")\n",
        "\n",
        "print(f\"\\nü§ñ Agents Executed:\")\n",
        "for agent in summary['agents_executed']:\n",
        "    print(f\"   ‚úì {agent}\")\n",
        "\n",
        "print(f\"\\nüìé Alerts Sent: {len(alert_tool.get_all_alerts())}\")\n",
        "\n",
        "print(f\"\\nüíæ Memory Bank:\")\n",
        "print(f\"   Total memories: {len(memory.get_all_memories())}\")\n",
        "print(f\"   Latest: {memory_id}\")\n",
        "\n",
        "metrics = logger.get_metrics_summary()\n",
        "print(f\"\\nüìä Performance Metrics:\")\n",
        "print(f\"   Total agent calls: {metrics['total_agent_calls']}\")\n",
        "print(f\"   Total tool calls: {metrics['total_tool_calls']}\")\n",
        "print(f\"   Execution time: {sum(metrics['avg_execution_times'].values()):.2f}s\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úÖ Integration Complete!\")\n",
        "print(\"‚úÖ Parallel execution working!\")\n",
        "print(\"‚úÖ Memory Bank storing analyses!\")\n",
        "print(f\"{'='*60}\")\n"
      ],
      "metadata": {
        "id": "p-ibPR6PzwbJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Test Reporter Agent\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from agents.reporter_agent import ReporterAgent\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Testing Reporter Agent\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "\n",
        "# ===== Step 1: Ingest =====\n",
        "print(\"Step 1: Loading data...\")\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "# ===== Step 2: Analyze =====\n",
        "print(\"\\nStep 2: Running analysis...\")\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# ===== Step 3: Detect Anomalies =====\n",
        "print(\"\\nStep 3: Detecting anomalies...\")\n",
        "anomaly_agent = AnomalyAgent(session, logger)\n",
        "anomaly_results = anomaly_agent.detect_anomalies(df)\n",
        "\n",
        "# ===== Step 4: Generate Report =====\n",
        "print(\"\\nStep 4: Generating PDF report...\")\n",
        "reporter_agent = ReporterAgent(session, logger)\n",
        "report_result = reporter_agent.generate_report()\n",
        "\n",
        "# ===== Display Results =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"REPORT GENERATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n‚úÖ Report Generated Successfully!\")\n",
        "print(f\"\\nüìÑ Report Details:\")\n",
        "print(f\"   Filename: {report_result['report_filename']}\")\n",
        "print(f\"   Path: {report_result['report_path']}\")\n",
        "print(f\"   Pages: {report_result['pages']}\")\n",
        "print(f\"   Generated: {report_result['generated_at']}\")\n",
        "\n",
        "print(f\"\\nüìä Report Contents:\")\n",
        "print(f\"   Dataset: {report_result['dataset']}\")\n",
        "print(f\"   Anomalies: {report_result['total_anomalies']}\")\n",
        "print(f\"   Insights: {report_result['total_insights']}\")\n",
        "print(f\"   Visualizations: {report_result['total_plots']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ PDF Report successfully created!\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Check if file exists\n",
        "import os\n",
        "if os.path.exists(report_result['report_path']):\n",
        "    file_size = os.path.getsize(report_result['report_path']) / 1024\n",
        "    print(f\"\\nüì¶ File size: {file_size:.2f} KB\")\n",
        "    print(f\"‚úì Report is ready for download!\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Report file not found at: {report_result['report_path']}\")\n"
      ],
      "metadata": {
        "id": "cQNBe3n1Ed8-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21: Integration Test - Full Pipeline\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from services.memory_bank import MemoryBank\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from agents.reporter_agent import ReporterAgent\n",
        "from tools.alert_tool import AlertTool, AlertSeverity\n",
        "from config import Config\n",
        "import time\n",
        "\n",
        "print(\"üöÄ END-TO-END INTEGRATION TEST\\n\")\n",
        "print(\"=\"*60)\n",
        "print(\"DataSense AI - Full Multi-Agent Analysis Pipeline\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize all services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "memory = MemoryBank()\n",
        "alert_tool = AlertTool(logger)\n",
        "\n",
        "# ===== PHASE 1: DATA INGESTION =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 1: DATA INGESTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_phase1 = time.time()\n",
        "\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "phase1_time = time.time() - start_phase1\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"‚úÖ Dataset Loaded\",\n",
        "    f\"{df.shape[0]:,} rows √ó {df.shape[1]} columns loaded successfully\",\n",
        "    AlertSeverity.INFO,\n",
        "    {'quality': profile['quality_metrics']['overall_quality']}\n",
        ")\n",
        "\n",
        "# ===== PHASE 2: DATA ANALYSIS =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: DATA ANALYSIS (EDA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_phase2 = time.time()\n",
        "\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "phase2_time = time.time() - start_phase2\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"‚úÖ Analysis Complete\",\n",
        "    f\"Generated {len(analysis_results['visualizations'])} plots, {len(analysis_results['insights'])} insights\",\n",
        "    AlertSeverity.INFO\n",
        ")\n",
        "\n",
        "# ===== PHASE 3: ANOMALY DETECTION (PARALLEL) =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 3: ANOMALY DETECTION (PARALLEL EXECUTION)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_phase3 = time.time()\n",
        "\n",
        "anomaly_agent = AnomalyAgent(session, logger)\n",
        "anomaly_results = anomaly_agent.detect_anomalies(df)\n",
        "\n",
        "phase3_time = time.time() - start_phase3\n",
        "\n",
        "# Alert for anomalies\n",
        "high_severity = [a for a in anomaly_results['anomalies'] if a.get('severity') in ['critical', 'high']]\n",
        "if high_severity:\n",
        "    alert_tool.send_alert(\n",
        "        \"‚ö†Ô∏è  High-Severity Anomalies\",\n",
        "        f\"Detected {len(high_severity)} critical/high anomalies\",\n",
        "        AlertSeverity.WARNING\n",
        "    )\n",
        "\n",
        "# ===== PHASE 4: REPORT GENERATION =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 4: REPORT GENERATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_phase4 = time.time()\n",
        "\n",
        "reporter_agent = ReporterAgent(session, logger)\n",
        "report_result = reporter_agent.generate_report()\n",
        "\n",
        "phase4_time = time.time() - start_phase4\n",
        "\n",
        "# Store in memory\n",
        "analysis_summary = {\n",
        "    'shape': df.shape,\n",
        "    'columns': df.columns.tolist(),\n",
        "    'anomalies_count': len(anomaly_results['anomalies']),\n",
        "    'quality_score': profile['quality_metrics']['completeness_score'],\n",
        "    'key_findings': [\n",
        "        f\"Detected {len(anomaly_results['anomalies'])} anomalies\",\n",
        "        f\"Quality: {profile['quality_metrics']['overall_quality']}\",\n",
        "        f\"Generated {len(analysis_results['visualizations'])} visualizations\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "memory_id = memory.store_analysis('sales.csv', analysis_summary)\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"‚úÖ Report Generated\",\n",
        "    f\"Created {report_result['pages']}-page PDF report\",\n",
        "    AlertSeverity.INFO,\n",
        "    {'filename': report_result['report_filename']}\n",
        ")\n",
        "\n",
        "# ===== FINAL SUMMARY =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL SUMMARY - ALL PHASES COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_time = phase1_time + phase2_time + phase3_time + phase4_time\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  EXECUTION TIMES:\")\n",
        "print(f\"   Phase 1 (Ingest):        {phase1_time:.2f}s\")\n",
        "print(f\"   Phase 2 (Analysis):      {phase2_time:.2f}s\")\n",
        "print(f\"   Phase 3 (Anomalies):     {phase3_time:.2f}s\")\n",
        "print(f\"   Phase 4 (Report):        {phase4_time:.2f}s\")\n",
        "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "print(f\"   TOTAL:                   {total_time:.2f}s\")\n",
        "\n",
        "# Session summary\n",
        "session_summary = session.get_session_summary()\n",
        "\n",
        "print(f\"\\nüìä ANALYSIS SUMMARY:\")\n",
        "print(f\"   Session ID: {session_summary['session_id']}\")\n",
        "print(f\"   Dataset: {session_summary['dataset_name']} ({session_summary['dataset_shape']})\")\n",
        "\n",
        "print(f\"\\nüìà FINDINGS:\")\n",
        "print(f\"   Total Insights: {session_summary['total_insights']}\")\n",
        "print(f\"   Total Anomalies: {session_summary['total_anomalies']}\")\n",
        "print(f\"   Total Plots: {session_summary['total_plots']}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  SEVERITY BREAKDOWN:\")\n",
        "for severity, count in session_summary['anomalies_by_severity'].items():\n",
        "    if count > 0:\n",
        "        print(f\"   {severity.upper()}: {count}\")\n",
        "\n",
        "print(f\"\\nü§ñ AGENTS EXECUTED:\")\n",
        "for agent in session_summary['agents_executed']:\n",
        "    print(f\"   ‚úì {agent}\")\n",
        "\n",
        "print(f\"\\nüìÑ REPORT DETAILS:\")\n",
        "print(f\"   Filename: {report_result['report_filename']}\")\n",
        "print(f\"   Pages: {report_result['pages']}\")\n",
        "print(f\"   Path: {report_result['report_path']}\")\n",
        "\n",
        "print(f\"\\nüíæ MEMORY BANK:\")\n",
        "print(f\"   Analysis ID: {memory_id}\")\n",
        "print(f\"   Total Stored: {len(memory.get_all_memories())}\")\n",
        "\n",
        "# Metrics\n",
        "metrics = logger.get_metrics_summary()\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
        "print(f\"   Total Agent Calls: {metrics['total_agent_calls']}\")\n",
        "print(f\"   Total Tool Calls: {metrics['total_tool_calls']}\")\n",
        "print(f\"   Total Errors: {metrics['total_errors']}\")\n",
        "\n",
        "print(f\"\\nüîî ALERTS SENT: {len(alert_tool.get_all_alerts())}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úÖ COMPLETE END-TO-END PIPELINE SUCCESSFUL!\")\n",
        "print(\"‚úÖ All 4 agents working together perfectly!\")\n",
        "print(\"‚úÖ Professional PDF report generated!\")\n",
        "print(f\"{'='*60}\\n\")\n"
      ],
      "metadata": {
        "id": "53a8I5m6qkgM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Final Test Run\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "test_files = [\n",
        "    'test_ingest_agent.py',\n",
        "    'test_analysis_agent.py',\n",
        "    'test_anomaly_agent.py',\n",
        "    'test_end_to_end.py'\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RUNNING ALL TESTS (FINAL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "passed = 0\n",
        "failed = 0\n",
        "\n",
        "for test_file in test_files:\n",
        "    print(f\"\\nRunning: {test_file}\")\n",
        "\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, f'/content/drive/MyDrive/DataSense_AI_Agent/tests/{test_file}'],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=120\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        passed += 1\n",
        "        print(f\"‚úÖ PASSED\")\n",
        "    else:\n",
        "        failed += 1\n",
        "        print(f\"‚ùå FAILED\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"RESULTS: {passed} PASSED, {failed} FAILED out of {len(test_files)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if failed == 0:\n",
        "    print(\"\\nüéâ ALL TESTS PASSED! Ready for Day 6!\")\n"
      ],
      "metadata": {
        "id": "ngkmJFNGg_Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 24: Evaluation Framework\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "class EvaluationFramework:\n",
        "    \"\"\"\n",
        "    Framework for collecting and analyzing human feedback\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.evaluations = []\n",
        "\n",
        "    def add_evaluation(self,\n",
        "                      dataset_name: str,\n",
        "                      dataset_clarity: int,  # 1-5\n",
        "                      insight_usefulness: int,  # 1-5\n",
        "                      anomaly_relevance: int,  # 1-5\n",
        "                      report_clarity: int,  # 1-5\n",
        "                      overall_satisfaction: int,  # 1-5\n",
        "                      comments: str = \"\"):\n",
        "        \"\"\"\n",
        "        Record one person's evaluation\n",
        "\n",
        "        Args:\n",
        "            dataset_name: Name of dataset tested\n",
        "            dataset_clarity: 1-5 rating\n",
        "            insight_usefulness: 1-5 rating\n",
        "            anomaly_relevance: 1-5 rating\n",
        "            report_clarity: 1-5 rating\n",
        "            overall_satisfaction: 1-5 rating\n",
        "            comments: Optional feedback\n",
        "        \"\"\"\n",
        "\n",
        "        evaluation = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'dataset': dataset_name,\n",
        "            'dataset_clarity': dataset_clarity,\n",
        "            'insight_usefulness': insight_usefulness,\n",
        "            'anomaly_relevance': anomaly_relevance,\n",
        "            'report_clarity': report_clarity,\n",
        "            'overall_satisfaction': overall_satisfaction,\n",
        "            'comments': comments\n",
        "        }\n",
        "\n",
        "        self.evaluations.append(evaluation)\n",
        "        print(f\"‚úÖ Evaluation recorded for {dataset_name}\")\n",
        "\n",
        "    def get_summary(self) -> dict:\n",
        "        \"\"\"\n",
        "        Calculate statistics from all evaluations\n",
        "        \"\"\"\n",
        "        if not self.evaluations:\n",
        "            return {'error': 'No evaluations recorded'}\n",
        "\n",
        "        df = pd.DataFrame(self.evaluations)\n",
        "\n",
        "        # Calculate averages\n",
        "        metrics = [\n",
        "            'dataset_clarity',\n",
        "            'insight_usefulness',\n",
        "            'anomaly_relevance',\n",
        "            'report_clarity',\n",
        "            'overall_satisfaction'\n",
        "        ]\n",
        "\n",
        "        summary = {\n",
        "            'total_evaluations': len(self.evaluations),\n",
        "            'average_scores': {},\n",
        "            'min_scores': {},\n",
        "            'max_scores': {}\n",
        "        }\n",
        "\n",
        "        for metric in metrics:\n",
        "            scores = df[metric].values\n",
        "            summary['average_scores'][metric] = round(sum(scores) / len(scores), 2)\n",
        "            summary['min_scores'][metric] = min(scores)\n",
        "            summary['max_scores'][metric] = max(scores)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print evaluation report\"\"\"\n",
        "        summary = self.get_summary()\n",
        "\n",
        "        if 'error' in summary:\n",
        "            print(summary['error'])\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"HUMAN EVALUATION REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"\\nüìä Total Evaluations: {summary['total_evaluations']}\")\n",
        "\n",
        "        print(f\"\\nüìà Average Scores (out of 5):\")\n",
        "\n",
        "        for metric, avg_score in summary['average_scores'].items():\n",
        "            # Visual bar\n",
        "            filled = int(avg_score)\n",
        "            bar = \"‚ñà\" * filled + \"‚ñë\" * (5 - filled)\n",
        "\n",
        "            metric_label = metric.replace('_', ' ').title()\n",
        "            print(f\"   {metric_label:.<30} {bar} {avg_score:.1f}/5\")\n",
        "\n",
        "        # Overall average\n",
        "        overall_avg = summary['average_scores']['overall_satisfaction']\n",
        "        print(f\"\\n   {'Overall Satisfaction':.<30} {overall_avg:.1f}/5\")\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "\n",
        "\n",
        "# Example usage: Add sample evaluations\n",
        "print(\"Creating Evaluation Framework...\\n\")\n",
        "\n",
        "evaluator = EvaluationFramework()\n",
        "\n",
        "# Simulated evaluations\n",
        "sample_evaluations = [\n",
        "    ('sales.csv', 5, 4, 4, 5, 4, 'Great tool, very clear insights'),\n",
        "    ('customer_data.csv', 4, 4, 3, 4, 4, 'Good but more granular anomalies would help'),\n",
        "    ('transactions.csv', 5, 5, 4, 5, 5, 'Excellent! Would definitely use again'),\n",
        "]\n",
        "\n",
        "print(\"Recording sample evaluations...\\n\")\n",
        "for dataset, clarity, usefulness, relevance, report, overall, comment in sample_evaluations:\n",
        "    evaluator.add_evaluation(\n",
        "        dataset_name=dataset,\n",
        "        dataset_clarity=clarity,\n",
        "        insight_usefulness=usefulness,\n",
        "        anomaly_relevance=relevance,\n",
        "        report_clarity=report,\n",
        "        overall_satisfaction=overall,\n",
        "        comments=comment\n",
        "    )\n",
        "\n",
        "# Print report\n",
        "evaluator.print_report()\n",
        "\n",
        "# Export to CSV\n",
        "evaluations_df = pd.DataFrame(evaluator.evaluations)\n",
        "evaluations_df.to_csv(\n",
        "    '/content/drive/MyDrive/DataSense_AI_Agent/outputs/evaluations.csv',\n",
        "    index=False\n",
        ")\n",
        "print(\"\\n‚úÖ Evaluations saved to: outputs/evaluations.csv\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w2lJgnUANKGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wKGb2XpYKuuH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPS60OPGRusy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}