{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1fL-T2SY0YNJltcGWMzdq5173sMxfEQhV",
      "authorship_tag": "ABX9TyO1OzwLPHbEXommFsP4GA5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdey26/DataSense_AI_Agent/blob/main/DataSense_AI_Agent_Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bNsqpOUjP4d-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set artifact output directory (portable)\n",
        "import os\n",
        "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"/tmp/datasense_outputs\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(\"Artifacts will be saved to:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "pbPs3fjaP-nw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes if you get the error while running the One-cell then just Cell 3 and\n",
        "then restart the session.                                                  \n",
        "(Runtime -> Restart Session or Ctrl + M )\n"
      ],
      "metadata": {
        "id": "sDP5qhn5PuWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# üîµ ONE-CELL DEMO with interactive upload (Colab)\n",
        "from IPython.display import FileLink, display\n",
        "import time, os, sys\n",
        "from google.colab import files   # works only in Colab\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "\n",
        "from services.session_service import SessionService\n",
        "from utils.logger import AgentLogger\n",
        "from services.memory_bank import MemoryBank\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from agents.reporter_agent import ReporterAgent\n",
        "from config import Config\n",
        "\n",
        "# Ensure OUTPUT_DIR exists (if not defined earlier)\n",
        "OUTPUT_DIR = os.getenv(\"OUTPUT_DIR\", \"/tmp/datasense_outputs\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"üöÄ Starting ONE-CELL FULL PIPELINE DEMO (Upload or fallback to sample)\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session_service = SessionService()\n",
        "memory = MemoryBank()\n",
        "\n",
        "print(f\"Session created: {session_service.session_id}\")\n",
        "print(\"Memory Bank initialized.\\n\")\n",
        "\n",
        "# --- Upload prompt ---\n",
        "print(\"üîΩ Upload a CSV file now (or cancel to use bundled sample):\")\n",
        "uploaded = files.upload()   # opens file picker in Colab\n",
        "\n",
        "if uploaded:\n",
        "    # take the first uploaded file\n",
        "    uploaded_name = list(uploaded.keys())[0]\n",
        "    saved_path = os.path.join(OUTPUT_DIR, uploaded_name)\n",
        "    # write bytes to OUTPUT_DIR\n",
        "    with open(saved_path, \"wb\") as f:\n",
        "        f.write(uploaded[uploaded_name])\n",
        "    sample_file_path = saved_path\n",
        "    print(f\"Uploaded file saved to: {sample_file_path}\")\n",
        "else:\n",
        "    # fallback to sample included in the repo\n",
        "    sample_file_path = f\"{Config.BASE_PATH}/sample_data/sales.csv\"\n",
        "    print(\"No file uploaded. Falling back to bundled sample:\", sample_file_path)\n",
        "\n",
        "# Validate path\n",
        "if not os.path.exists(sample_file_path):\n",
        "    raise FileNotFoundError(f\"Dataset path not found: {sample_file_path}\")\n",
        "\n",
        "# --- Pipeline Steps ---\n",
        "print(\"\\n============================================================\")\n",
        "print(\"Step 1: Running Ingest Agent...\")\n",
        "print(\"============================================================\")\n",
        "start_ingest = time.time()\n",
        "ingest_agent = IngestAgent(session_service, logger)\n",
        "# Try to use ingest_csv(path). If your IngestAgent expects a DataFrame, read and call ingest_dataframe.\n",
        "try:\n",
        "    profile = ingest_agent.ingest_csv(sample_file_path)\n",
        "except AttributeError:\n",
        "    # fallback: read as pandas and call ingest_dataframe if available\n",
        "    import pandas as pd\n",
        "    df_local = pd.read_csv(sample_file_path)\n",
        "    if hasattr(ingest_agent, \"ingest_dataframe\"):\n",
        "        profile = ingest_agent.ingest_dataframe(df_local)\n",
        "    else:\n",
        "        # as last resort, store df in session and continue\n",
        "        session_service.store(session_service.session_id, \"df\", df_local)\n",
        "        profile = {\"filename\": os.path.basename(sample_file_path), \"quality_metrics\": {\"completeness_score\": None, \"overall_quality\": None}}\n",
        "        print(\"Warning: IngestAgent.ingest_csv not available; stored DataFrame directly in session.\")\n",
        "\n",
        "df = ingest_agent.get_dataframe() if hasattr(ingest_agent, \"get_dataframe\") else session_service.get(session_service.session_id, \"df\")\n",
        "ingest_time = time.time() - start_ingest\n",
        "print(f\"Ingestion done ({ingest_time:.2f}s). Dataset: {profile.get('filename')} ({df.shape[0]} rows)\")\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\"Step 2: Running Analysis Agent...\")\n",
        "print(\"============================================================\")\n",
        "start_analysis = time.time()\n",
        "analysis_agent_instance = AnalysisAgent(session_service, logger)\n",
        "analysis_res = analysis_agent_instance.analyze(df)\n",
        "analysis_time = time.time() - start_analysis\n",
        "print(f\"Analysis done ({analysis_time:.2f}s). Insights: {len(analysis_res.get('insights', []))}, Visualizations: {len(analysis_res.get('visualizations', []))}\")\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\"Step 3: Running Anomaly Agent...\")\n",
        "print(\"============================================================\")\n",
        "start_anomaly = time.time()\n",
        "anomaly_agent_instance = AnomalyAgent(session_service, logger)\n",
        "anomaly_res = anomaly_agent_instance.detect_anomalies(df)\n",
        "anomaly_time = time.time() - start_anomaly\n",
        "print(f\"Anomaly detection done ({anomaly_time:.2f}s). Total Anomalies: {anomaly_res.get('total_anomalies', 0)}\")\n",
        "\n",
        "# Store analysis in memory\n",
        "analysis_summary = {\n",
        "    'shape': df.shape,\n",
        "    'columns': df.columns.tolist(),\n",
        "    'anomalies_count': anomaly_res.get('total_anomalies', 0),\n",
        "    'quality_score': profile.get('quality_metrics', {}).get('completeness_score'),\n",
        "    'key_findings': [\n",
        "        f\"Detected {anomaly_res.get('total_anomalies', 0)} anomalies\",\n",
        "        f\"Quality: {profile.get('quality_metrics', {}).get('overall_quality')}\",\n",
        "        f\"Generated {len(analysis_res.get('visualizations', []))} visualizations\"\n",
        "    ]\n",
        "}\n",
        "memory_id = memory.store_analysis(profile.get('filename'), analysis_summary)\n",
        "print(f\"Analysis stored in Memory Bank with ID: {memory_id}\")\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\"Step 4: Compiling Report...\")\n",
        "print(\"============================================================\")\n",
        "start_report = time.time()\n",
        "reporter_agent = ReporterAgent(session_service, logger)\n",
        "report = reporter_agent.generate_report()\n",
        "report_time = time.time() - start_report\n",
        "print(f\"Report generated ({report_time:.2f}s). File: {report.get('report_filename')}\")\n",
        "\n",
        "# show download link for convenience\n",
        "if report.get('report_path') and os.path.exists(report.get('report_path')):\n",
        "    print(f\"\\n‚úÖ Full Pipeline Demo Complete! Report available at: {report.get('report_path')}\")\n",
        "    display(FileLink(report.get(\"report_path\")))\n",
        "else:\n",
        "    # show files in OUTPUT_DIR to help debugging\n",
        "    print(\"\\n‚ùå Report generation failed or report not found. Listing OUTPUT_DIR for debugging:\")\n",
        "    print(os.listdir(OUTPUT_DIR))\n",
        "\n",
        "print(\"\\n============================================================\")\n",
        "print(\"TOTAL PIPELINE DURATION: \", round(ingest_time + analysis_time + anomaly_time + report_time, 2), \"s\")\n",
        "print(\"============================================================\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "collapsed": true,
        "id": "3qaw0LlEFfiz",
        "outputId": "05f87f00-f520-4128-bbd7-c9b347b08135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'services'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3580033267.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_service\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSessionService\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAgentLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_bank\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMemoryBank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'services'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e6T_tSU6PsJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 1: Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "83W0DWvr8QBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: Setup API Key (SECURE)\n",
        "from google.colab import userdata\n",
        "\n",
        "# Now code can use it:\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "print(\"‚úÖ API Key loaded securely!\")"
      ],
      "metadata": {
        "id": "-b_eGkqQ9k15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 3: Install Required Packages\n",
        "\n",
        "!pip install -q google-generativeai  # Google's Gemini AI\n",
        "!pip install -q langgraph            # For multi-agent orchestration\n",
        "!pip install -q pandas               # For data manipulation\n",
        "!pip install -q matplotlib           # For creating charts\n",
        "!pip install -q seaborn              # For beautiful charts\n",
        "!pip install -q fpdf                 # For creating PDF reports\n",
        "!pip install -q scikit-learn         # For data analysis tools\n",
        "!pip install -q scipy                # For scientific computing\n",
        "\n",
        "print(\"‚úÖ All packages installed!\")\n"
      ],
      "metadata": {
        "id": "nj5uWP2G-RbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f51a86d2-b26b-470b-a3d6-4eaf899ffa87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All packages installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 5: Initialize Gemini\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key from secrets\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "# Configure the AI\n",
        "genai.configure(api_key=API_KEY)\n",
        "\n",
        "# Test it works\n",
        "model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "response = model.generate_content(\"Say 'Hello! I'm ready to help analyze data!'\")\n",
        "print(response.text)\n"
      ],
      "metadata": {
        "id": "m1lhBTtJIm9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 6: Create Sample Sales Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "from config import Config\n",
        "\n",
        "# Set random seed for reproducibility (same data every time)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create date range (Jan 1 to Nov 25, 2024)\n",
        "dates = pd.date_range(start='2024-01-01', end='2024-11-25', freq='D')\n",
        "num_days = len(dates)\n",
        "\n",
        "print(f\"Creating {num_days} days of sales data...\")\n",
        "\n",
        "# Create the dataset\n",
        "sales_data = pd.DataFrame({\n",
        "    # Date column\n",
        "    'date': dates,\n",
        "\n",
        "    # Product IDs (5 different products)\n",
        "    'product_id': np.random.choice(\n",
        "        ['P001', 'P002', 'P003', 'P004', 'P005'],\n",
        "        num_days\n",
        "    ),\n",
        "\n",
        "    # Sales amount (normally distributed around $5000)\n",
        "    # np.random.normal(mean, std_deviation, count)\n",
        "    'sales_amount': np.random.normal(5000, 1500, num_days),\n",
        "\n",
        "    # Quantity sold (Poisson distribution, average 50 units)\n",
        "    'quantity_sold': np.random.poisson(50, num_days),\n",
        "\n",
        "    # Region (4 regions)\n",
        "    'region': np.random.choice(\n",
        "        ['North', 'South', 'East', 'West'],\n",
        "        num_days\n",
        "    ),\n",
        "\n",
        "    # Customer segment\n",
        "    'customer_segment': np.random.choice(\n",
        "        ['Retail', 'Wholesale', 'Online'],\n",
        "        num_days\n",
        "    )\n",
        "})\n",
        "\n",
        "# ‚ñÑ Add realistic anomalies (this is what we'll detect!)\n",
        "print(\"Adding anomalies...\")\n",
        "\n",
        "# Pick 15 random days to be anomalies\n",
        "anomaly_indices = np.random.choice(num_days, 15, replace=False)\n",
        "\n",
        "# Create spike anomalies (sudden sales increase)\n",
        "spike_indices = anomaly_indices[:10]\n",
        "sales_data.loc[spike_indices, 'sales_amount'] *= 3\n",
        "print(f\"  ‚ÜóÔ∏è  Added {len(spike_indices)} spike anomalies (3x normal sales)\")\n",
        "\n",
        "# Create drop anomalies (sudden sales decrease)\n",
        "drop_indices = anomaly_indices[10:]\n",
        "sales_data.loc[drop_indices, 'sales_amount'] *= 0.1\n",
        "print(f\"  ‚ÜòÔ∏è  Added {len(drop_indices)} drop anomalies (10% normal sales)\")\n",
        "\n",
        "# Add missing values (realistic data quality issues)\n",
        "missing_indices = np.random.choice(num_days, 10, replace=False)\n",
        "sales_data.loc[missing_indices, 'customer_segment'] = np.nan\n",
        "print(f\"  ‚ùì Added {len(missing_indices)} missing values\")\n",
        "\n",
        "# Save to Google Drive\n",
        "output_path = f'{Config.BASE_PATH}/sample_data/sales.csv'\n",
        "sales_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\n‚úÖ Created: sales.csv\")\n",
        "print(f\"   Shape: {sales_data.shape[0]} rows √ó {sales_data.shape[1]} columns\")\n",
        "print(f\"   Saved to: {output_path}\")\n",
        "\n",
        "# Show preview\n",
        "print(\"\\nüìä Preview (first 5 rows):\")\n",
        "print(sales_data.head())\n"
      ],
      "metadata": {
        "id": "hg6jbg9zSuB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 7: Create Transaction Dataset\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "from config import Config\n",
        "\n",
        "print(\"Creating transaction dataset...\")\n",
        "\n",
        "transactions = pd.DataFrame({\n",
        "    'transaction_id': [f'T{i:05d}' for i in range(1, 1001)],  # T00001, T00002, ...\n",
        "    'timestamp': pd.date_range(start='2024-01-01', periods=1000, freq='h'),\n",
        "    'user_id': np.random.randint(1000, 5000, 1000),\n",
        "    'amount': np.random.exponential(100, 1000),  # Most small, few large\n",
        "    'payment_method': np.random.choice(\n",
        "        ['Credit', 'Debit', 'UPI', 'Cash'],\n",
        "        1000\n",
        "    ),\n",
        "    'status': np.random.choice(\n",
        "        ['Success', 'Failed', 'Pending'],\n",
        "        1000,\n",
        "        p=[0.9, 0.08, 0.02]  # 90% success, 8% failed, 2% pending\n",
        "    )\n",
        "})\n",
        "\n",
        "transactions.to_csv(f'{Config.BASE_PATH}/sample_data/transactions.csv', index=False)\n",
        "print(f\"‚úÖ Created: transactions.csv ({transactions.shape[0]} rows)\")"
      ],
      "metadata": {
        "id": "GpcESvHWXCID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 8: Create Customer Dataset\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "from config import Config\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Creating customer dataset...\")\n",
        "\n",
        "customers = pd.DataFrame({\n",
        "    'customer_id': range(1, 501),\n",
        "    'age': np.random.randint(18, 75, 500),\n",
        "    'income': np.random.lognormal(10.5, 0.5, 500),  # Income distribution\n",
        "    'credit_score': np.random.randint(300, 850, 500),\n",
        "    'tenure_months': np.random.randint(1, 120, 500),\n",
        "    'churn': np.random.choice([0, 1], 500, p=[0.85, 0.15])  # 15% churned\n",
        "})\n",
        "\n",
        "customers.to_csv(f'{Config.BASE_PATH}/sample_data/customer_data.csv', index=False)\n",
        "print(f\"‚úÖ Created: customer_data.csv ({customers.shape[0]} rows)\")\n",
        "\n",
        "print(\"\\nüéâ All sample datasets created!\")"
      ],
      "metadata": {
        "id": "xHZVd0a9XKxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9: Test Config\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "from config import Config\n",
        "\n",
        "print(\"Testing configuration...\")\n",
        "print(f\"‚úì Base path: {Config.BASE_PATH}\")\n",
        "print(f\"‚úì Model: {Config.GEMINI_MODEL}\")\n",
        "print(f\"‚úì Anomaly threshold: {Config.ANOMALY_THRESHOLD}\")\n",
        "print(\"\\n‚úÖ Config works!\")\n"
      ],
      "metadata": {
        "id": "t06r-lNahDBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 10: Test Logger\n",
        "from utils.logger import AgentLogger\n",
        "from config import Config\n",
        "\n",
        "# Create logger\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "\n",
        "# Simulate agent activity\n",
        "logger.log_agent_start('TestAgent', {'test': 'data'})\n",
        "import time\n",
        "time.sleep(1)  # Simulate work\n",
        "logger.log_agent_end('TestAgent', {'result': 'success'}, 1.0)\n",
        "\n",
        "# Simulate tool call\n",
        "logger.log_tool_call('TestTool', {'param': 'value'}, 'Tool output')\n",
        "\n",
        "# Print summary\n",
        "logger.print_summary()\n"
      ],
      "metadata": {
        "id": "29ojLB6ttar7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Test Session Service\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "import os, sys\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "\n",
        "# Create a session\n",
        "session = SessionService()\n",
        "\n",
        "# Simulate Ingest Agent storing dataset info\n",
        "session.store_dataset_info({\n",
        "    'filename': 'sales.csv',\n",
        "    'shape': (330, 6),\n",
        "    'missing_percentage': {'date': 0, 'sales_amount': 0.5}\n",
        "})\n",
        "\n",
        "# Simulate Anomaly Agent finding issues\n",
        "session.add_anomaly({\n",
        "    'type': 'spike',\n",
        "    'column': 'sales_amount',\n",
        "    'row': 42,\n",
        "    'value': 15000,\n",
        "    'severity': 'high'\n",
        "})\n",
        "\n",
        "session.add_anomaly({\n",
        "    'type': 'outlier',\n",
        "    'column': 'quantity_sold',\n",
        "    'row': 105,\n",
        "    'value': 500,\n",
        "    'severity': 'medium'\n",
        "})\n",
        "\n",
        "# Simulate Analysis Agent storing insights\n",
        "session.add_insight(\n",
        "    'North region has 2.5x higher average sales',\n",
        "    'regional_analysis'\n",
        ")\n",
        "\n",
        "# Print summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "summary = session.get_session_summary()\n",
        "for key, value in summary.items():\n",
        "    print(f\"  {key}: {value}\")\n"
      ],
      "metadata": {
        "id": "W1YlYNS0vE26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 12: Test Agent Communication Protocol\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "from agents import AgentRole, AgentMessage, BaseAgent\n",
        "\n",
        "# Create a test message\n",
        "message = AgentMessage(\n",
        "    sender=AgentRole.INGEST,\n",
        "    receiver=AgentRole.ANALYSIS,\n",
        "    message_type=\"request\",\n",
        "    payload={\n",
        "        'dataset_name': 'sales.csv',\n",
        "        'rows': 330,\n",
        "        'columns': ['date', 'sales', 'region']\n",
        "    },\n",
        "    timestamp=\"2024-11-27T01:23:45\",\n",
        "    session_id=\"20241127_012345\"\n",
        ")\n",
        "\n",
        "# Show the message\n",
        "print(\"Message created:\")\n",
        "print(message.to_json())\n",
        "\n",
        "# Convert back from dict\n",
        "message_dict = message.to_dict()\n",
        "reconstructed = AgentMessage.from_dict(message_dict)\n",
        "\n",
        "print(\"\\n‚úÖ Message conversion successful!\")\n",
        "print(f\"Sender: {reconstructed.sender.value}\")\n",
        "print(f\"Receiver: {reconstructed.receiver.value}\")\n",
        "print(f\"Payload: {reconstructed.payload}\")\n"
      ],
      "metadata": {
        "id": "5dhZ4wb_xPg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 13: Test Complete Pipeline\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "from config import Config\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "\n",
        "print(\"üöÄ Starting Integration Test\\n\")\n",
        "\n",
        "# ===== Step 1: Initialize Services =====\n",
        "print(\"Step 1: Initializing services...\")\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "print(\"‚úì Logger and Session created\\n\")\n",
        "\n",
        "# ===== Step 2: Create Ingest Agent =====\n",
        "print(\"Step 2: Creating Ingest Agent...\")\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "print(\"‚úì Ingest Agent created\\n\")\n",
        "\n",
        "# ===== Step 3: Ingest Dataset =====\n",
        "print(\"Step 3: Ingesting dataset...\")\n",
        "profile = ingest_agent.ingest_csv(\n",
        "    f'{Config.BASE_PATH}/sample_data/sales.csv'\n",
        ")\n",
        "print(\"‚úì Dataset ingested\\n\")\n",
        "\n",
        "# ===== Step 4: Display Results =====\n",
        "print(\"=\"*60)\n",
        "print(\"INGEST AGENT RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Dataset: {profile['filename']}\")\n",
        "print(f\"üìè Shape: {profile['shape'][0]:,} rows √ó {profile['shape'][1]} columns\")\n",
        "print(f\"üíæ Memory: {profile['memory_usage_mb']} MB\")\n",
        "\n",
        "print(f\"\\n‚úÖ Quality: {profile['quality_metrics']['overall_quality']}\")\n",
        "print(f\"üìà Completeness: {profile['quality_metrics']['completeness_score']}%\")\n",
        "print(f\"üîÑ Duplicates: {profile['quality_metrics']['duplicate_rows']}\")\n",
        "\n",
        "print(f\"\\nüìã Columns: {', '.join(profile['columns'])}\")\n",
        "print(f\"üìÖ Date Columns: {', '.join(profile['date_columns'])}\")\n",
        "\n",
        "print(f\"\\nü§ñ AI Analysis:\")\n",
        "print(f\"   Type: {profile['llm_insights']['dataset_type']}\")\n",
        "print(f\"   Recommended: {', '.join(profile['llm_insights']['recommended_analysis'][:2])}\")\n",
        "\n",
        "if profile['llm_insights']['potential_issues']:\n",
        "    print(f\"   Issues: {', '.join(profile['llm_insights']['potential_issues'][:2])}\")\n",
        "\n",
        "# ===== Step 5: Show Session State =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION STATE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary = session.get_session_summary()\n",
        "print(f\"\\n Session ID: {summary['session_id']}\")\n",
        "print(f\"   Duration: {summary['duration_seconds']:.2f}s\")\n",
        "print(f\"   Dataset: {summary['dataset_name']}\")\n",
        "print(f\"   Agents: {', '.join(summary['agents_executed'])}\")\n",
        "\n",
        "# ===== Step 6: Show Metrics =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"EXECUTION METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "metrics = logger.get_metrics_summary()\n",
        "logger.print_summary()\n",
        "\n",
        "print(\"\\n‚úÖ Integration Test Complete!\")\n"
      ],
      "metadata": {
        "id": "E10NUcSJ0sSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 14: Test Code Executor\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils.logger import AgentLogger\n",
        "from tools.code_executor import CodeExecutor\n",
        "from config import Config\n",
        "\n",
        "# Create logger\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "\n",
        "# Create executor\n",
        "executor = CodeExecutor(logger)\n",
        "\n",
        "# Create sample dataframe\n",
        "df = pd.DataFrame({\n",
        "    'Product': ['A', 'B', 'C', 'A', 'B'],\n",
        "    'Sales': [1000, 1500, 2000, 1200, 1800],\n",
        "    'Region': ['North', 'South', 'East', 'North', 'South']\n",
        "})\n",
        "\n",
        "# Test 1: Simple calculation\n",
        "print(\"=\" * 60)\n",
        "print(\"TEST 1: Simple Calculation\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "code1 = \"\"\"\n",
        "print(\"Total Sales:\", df['Sales'].sum())\n",
        "print(\"Average Sales:\", df['Sales'].mean())\n",
        "print(\"Max Sales:\", df['Sales'].max())\n",
        "\n",
        "result = df.groupby('Region')['Sales'].sum()\n",
        "\"\"\"\n",
        "\n",
        "output1 = executor.run_code(code1, extra_context={'df': df})\n",
        "\n",
        "print(f\"‚úÖ Success: {output1['success']}\")\n",
        "print(f\"Output:\\n{output1['stdout']}\")\n",
        "if output1['result'] is not None:\n",
        "    print(f\"Result:\\n{output1['result']}\")\n",
        "\n",
        "# Test 2: Create visualization\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEST 2: Create Visualization\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "code2 = \"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(df['Product'], df['Sales'], color='steelblue')\n",
        "plt.title('Sales by Product')\n",
        "plt.xlabel('Product')\n",
        "plt.ylabel('Sales')\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\"\"\"\n",
        "\n",
        "output2 = executor.run_code(\n",
        "    code2,\n",
        "    extra_context={'df': df},\n",
        "    plot_filename='test_bar_chart.png'\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Success: {output2['success']}\")\n",
        "print(f\"Plot saved at: {output2['plot_path']}\")\n",
        "\n",
        "# Test 3: Error handling\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TEST 3: Error Handling\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "code3 = \"\"\"\n",
        "# This will cause an error\n",
        "result = df['NonExistentColumn'].sum()\n",
        "\"\"\"\n",
        "\n",
        "output3 = executor.run_code(code3, extra_context={'df': df})\n",
        "\n",
        "print(f\"‚ùå Success: {output3['success']}\")\n",
        "print(f\"Error:\\n{output3['error']}\")\n",
        "\n",
        "print(\"\\n‚úÖ Code Executor tests complete!\")\n"
      ],
      "metadata": {
        "id": "IBFSx9pp8J5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 15: Test Analysis Agent\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Testing Analysis Agent\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "\n",
        "# Load data with Ingest Agent\n",
        "print(\"Step 1: Loading data...\")\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "print(\"\\nStep 2: Running analysis...\")\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Dataset: {analysis_results['dataset_shape']}\")\n",
        "print(f\"üìà Numeric columns: {', '.join(analysis_results['numeric_columns'][:3])}\")\n",
        "print(f\"üìù Categorical columns: {', '.join(analysis_results['categorical_columns'][:2])}\")\n",
        "\n",
        "print(f\"\\nüìä Visualizations created: {len(analysis_results['visualizations'])}\")\n",
        "for viz in analysis_results['visualizations']:\n",
        "    print(f\"  ‚Ä¢ {viz['type']}: {viz['description']}\")\n",
        "\n",
        "print(f\"\\nüí° Insights: {len(analysis_results['insights'])}\")\n",
        "for insight in analysis_results['insights']:\n",
        "    print(f\"  ‚Ä¢ [{insight['severity']}] {insight['text']}\")\n",
        "\n",
        "# Show session state\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION STATE\")\n",
        "print(\"=\"*60)\n",
        "summary = session.get_session_summary()\n",
        "print(f\"\\nTotal insights: {summary['total_insights']}\")\n",
        "print(f\"Total plots: {summary['total_plots']}\")\n",
        "print(f\"Agents executed: {', '.join(summary['agents_executed'])}\")\n",
        "\n",
        "print(\"\\n‚úÖ Analysis Agent test complete!\")\n"
      ],
      "metadata": {
        "id": "_AcO0BHK_kdg",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 16: Integration Test\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from tools.alert_tool import AlertTool, AlertSeverity\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Integration Test\\n\")\n",
        "\n",
        "# ===== Initialize all services =====\n",
        "print(\"Initializing services...\")\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "alert_tool = AlertTool(logger)\n",
        "\n",
        "# ===== Step 1: Ingest Data =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: Data Ingestion\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"Data Loaded Successfully\",\n",
        "    f\"Loaded {df.shape[0]:,} rows √ó {df.shape[1]} columns\",\n",
        "    AlertSeverity.INFO,\n",
        "    {'dataset': 'sales.csv', 'shape': df.shape}\n",
        ")\n",
        "\n",
        "# ===== Step 2: Analyze Data =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: Data Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# Alert if quality issues found\n",
        "quality = profile['quality_metrics']\n",
        "if quality['overall_quality'] != 'Good':\n",
        "    alert_tool.send_alert(\n",
        "        \"Data Quality Alert\",\n",
        "        f\"Quality level: {quality['overall_quality']}\",\n",
        "        AlertSeverity.WARNING\n",
        "    )\n",
        "\n",
        "# ===== Step 3: Generate Report =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary = session.get_session_summary()\n",
        "\n",
        "print(f\"\\nüìä Session Summary:\")\n",
        "print(f\"   ID: {summary['session_id']}\")\n",
        "print(f\"   Duration: {summary['duration_seconds']:.2f}s\")\n",
        "print(f\"   Dataset: {summary['dataset_name']} ({summary['dataset_shape']})\")\n",
        "\n",
        "print(f\"\\nüìà Analysis Performed:\")\n",
        "print(f\"   Anomalies detected: {summary['total_anomalies']}\")\n",
        "print(f\"   Plots generated: {summary['total_plots']}\")\n",
        "print(f\"   Insights found: {summary['total_insights']}\")\n",
        "\n",
        "print(f\"\\nü§ñ Agents Executed:\")\n",
        "for agent in summary['agents_executed']:\n",
        "    print(f\"   ‚Ä¢ {agent}\")\n",
        "\n",
        "print(f\"\\nüîî Alerts Sent: {len(alert_tool.get_all_alerts())}\")\n",
        "for alert in alert_tool.get_all_alerts():\n",
        "    print(f\"   ‚Ä¢ [{alert['severity']}] {alert['title']}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úÖ Day 2 Integration Complete!\")\n",
        "print(f\"{'='*60}\")\n"
      ],
      "metadata": {
        "id": "QJUekvYXoKlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 17: Test Parallel Anomaly Detection\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Testing Parallel Anomaly Detection\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "\n",
        "# Load data\n",
        "print(\"Step 1: Loading data...\")\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "# Run analysis\n",
        "print(\"\\nStep 2: Running analysis...\")\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# Run anomaly detection (PARALLEL)\n",
        "print(\"\\nStep 3: Detecting anomalies (PARALLEL execution)...\")\n",
        "anomaly_agent = AnomalyAgent(session, logger)\n",
        "anomaly_results = anomaly_agent.detect_anomalies(df)\n",
        "\n",
        "# Display results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANOMALY DETECTION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüéØ Total Anomalies: {anomaly_results['total_anomalies']}\")\n",
        "\n",
        "print(f\"\\nüìä By Detection Method:\")\n",
        "for method, count in anomaly_results['anomalies_by_method'].items():\n",
        "    print(f\"   {method}: {count}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  By Severity:\")\n",
        "for severity, count in anomaly_results['severity_breakdown'].items():\n",
        "    if count > 0:\n",
        "        print(f\"   {severity.upper()}: {count}\")\n",
        "\n",
        "print(f\"\\nüìã Top 5 Anomalies:\")\n",
        "for i, anomaly in enumerate(anomaly_results['anomalies'][:5], 1):\n",
        "    print(f\"   {i}. [{anomaly['severity']}] {anomaly['type']} - {anomaly.get('description', 'N/A')}\")\n",
        "\n",
        "# Show session state\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SESSION STATE\")\n",
        "print(\"=\"*60)\n",
        "summary = session.get_session_summary()\n",
        "print(f\"\\nTotal anomalies stored: {summary['total_anomalies']}\")\n",
        "print(f\"Agents executed: {', '.join(summary['agents_executed'])}\")\n",
        "\n",
        "print(\"\\n‚úÖ Parallel anomaly detection test complete!\")\n"
      ],
      "metadata": {
        "id": "d6IUcNxUwzG1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 18: Test Memory Bank\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "from services.memory_bank import MemoryBank\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Testing Memory Bank\\n\")\n",
        "\n",
        "# Create memory bank\n",
        "memory = MemoryBank()\n",
        "\n",
        "# Store an analysis\n",
        "analysis_1 = {\n",
        "    'shape': (330, 6),\n",
        "    'columns': ['date', 'product_id', 'sales_amount', 'quantity_sold', 'region'],\n",
        "    'anomalies_count': 25,\n",
        "    'quality_score': 96.97,\n",
        "    'key_findings': [\n",
        "        'Strong correlation between sales and quantity',\n",
        "        'North region has higher sales',\n",
        "        '25 anomalies detected'\n",
        "    ]\n",
        "}\n",
        "\n",
        "memory_id_1 = memory.store_analysis('sales.csv', analysis_1)\n",
        "print(f\"‚úì Stored analysis: {memory_id_1}\")\n",
        "\n",
        "# Simulate a second analysis\n",
        "print(\"\\nWaiting 2 seconds...\")\n",
        "import time\n",
        "time.sleep(2)\n",
        "\n",
        "analysis_2 = {\n",
        "    'shape': (330, 6),\n",
        "    'columns': ['date', 'product_id', 'sales_amount', 'quantity_sold', 'region'],\n",
        "    'anomalies_count': 18,\n",
        "    'quality_score': 97.5,\n",
        "    'key_findings': [\n",
        "        'Similar patterns to previous analysis',\n",
        "        'Fewer anomalies this time',\n",
        "        'Improved data quality'\n",
        "    ]\n",
        "}\n",
        "\n",
        "memory_id_2 = memory.store_analysis('sales.csv', analysis_2)\n",
        "print(f\"‚úì Stored analysis: {memory_id_2}\")\n",
        "\n",
        "# Find similar analyses\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MEMORY BANK SEARCH\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "similar = memory.find_similar_analyses('sales.csv', max_results=5)\n",
        "print(f\"\\nFound {len(similar)} previous analyses of sales.csv\")\n",
        "for i, mem in enumerate(similar, 1):\n",
        "    print(f\"  {i}. {mem['memory_id']} ({mem['created_at']})\")\n",
        "\n",
        "# Compare\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "comparison = memory.get_comparison_summary(analysis_2, 'sales.csv')\n",
        "print(f\"\\nCurrent anomalies: {comparison['current_anomalies']}\")\n",
        "print(f\"Previous analyses: {comparison['previous_analyses']}\")\n",
        "\n",
        "for comp in comparison['comparisons']:\n",
        "    print(f\"\\nPrevious ({comp['date']}):\")\n",
        "    print(f\"  Anomalies: {comp['previous_anomalies']}\")\n",
        "    print(f\"  Change: {comp['change']:+d} ({comp['trend']})\")\n",
        "\n",
        "print(\"\\n‚úÖ Memory Bank test complete!\")\n"
      ],
      "metadata": {
        "id": "Th9wcXVByxBM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 19: Integration Test\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from services.memory_bank import MemoryBank\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from tools.alert_tool import AlertTool, AlertSeverity\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Integration Test\\n\")\n",
        "\n",
        "# Initialize all services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "memory = MemoryBank()\n",
        "alert_tool = AlertTool(logger)\n",
        "\n",
        "# ===== PHASE 1: Data Ingestion =====\n",
        "print(\"=\"*60)\n",
        "print(\"PHASE 1: Data Ingestion\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"Dataset Loaded\",\n",
        "    f\"{df.shape[0]} rows √ó {df.shape[1]} columns\",\n",
        "    AlertSeverity.INFO\n",
        ")\n",
        "\n",
        "# ===== PHASE 2: Analysis =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: Data Analysis\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# ===== PHASE 3: Anomaly Detection (PARALLEL) =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 3: Anomaly Detection (PARALLEL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "anomaly_agent = AnomalyAgent(session, logger)\n",
        "anomaly_results = anomaly_agent.detect_anomalies(df)\n",
        "\n",
        "# Send alerts for high-severity anomalies\n",
        "high_severity = [a for a in anomaly_results['anomalies']\n",
        "                 if a.get('severity') in ['critical', 'high']]\n",
        "\n",
        "if high_severity:\n",
        "    alert_tool.send_alert(\n",
        "        f\"High-Severity Anomalies Detected\",\n",
        "        f\"Found {len(high_severity)} critical/high anomalies\",\n",
        "        AlertSeverity.WARNING,\n",
        "        {'count': len(high_severity), 'sample': high_severity[0]}\n",
        "    )\n",
        "\n",
        "# ===== PHASE 4: Memory Storage =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 4: Storing Analysis in Memory\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "analysis_summary = {\n",
        "    'shape': df.shape,\n",
        "    'columns': df.columns.tolist(),\n",
        "    'anomalies_count': len(anomaly_results['anomalies']),\n",
        "    'quality_score': profile['quality_metrics']['completeness_score'],\n",
        "    'key_findings': [\n",
        "        f\"Detected {len(anomaly_results['anomalies'])} anomalies\",\n",
        "        f\"Quality: {profile['quality_metrics']['overall_quality']}\",\n",
        "        f\"Generated {len(analysis_results['visualizations'])} visualizations\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "memory_id = memory.store_analysis('sales.csv', analysis_summary)\n",
        "print(f\"‚úì Analysis stored in memory: {memory_id}\")\n",
        "\n",
        "# ===== FINAL REPORT =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL REPORT - DAY 3 COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "summary = session.get_session_summary()\n",
        "\n",
        "print(f\"\\nüìä Session: {summary['session_id']}\")\n",
        "print(f\"   Duration: {summary['duration_seconds']:.2f}s\")\n",
        "print(f\"   Dataset: {summary['dataset_name']} {summary['dataset_shape']}\")\n",
        "\n",
        "print(f\"\\nüìà Analysis Results:\")\n",
        "print(f\"   Insights: {summary['total_insights']}\")\n",
        "print(f\"   Plots: {summary['total_plots']}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  Anomaly Detection:\")\n",
        "print(f\"   Total: {anomaly_results['total_anomalies']}\")\n",
        "for severity, count in anomaly_results['severity_breakdown'].items():\n",
        "    if count > 0:\n",
        "        print(f\"   {severity.upper()}: {count}\")\n",
        "\n",
        "print(f\"\\nü§ñ Agents Executed:\")\n",
        "for agent in summary['agents_executed']:\n",
        "    print(f\"   ‚úì {agent}\")\n",
        "\n",
        "print(f\"\\nüìé Alerts Sent: {len(alert_tool.get_all_alerts())}\")\n",
        "\n",
        "print(f\"\\nüíæ Memory Bank:\")\n",
        "print(f\"   Total memories: {len(memory.get_all_memories())}\")\n",
        "print(f\"   Latest: {memory_id}\")\n",
        "\n",
        "metrics = logger.get_metrics_summary()\n",
        "print(f\"\\nüìä Performance Metrics:\")\n",
        "print(f\"   Total agent calls: {metrics['total_agent_calls']}\")\n",
        "print(f\"   Total tool calls: {metrics['total_tool_calls']}\")\n",
        "print(f\"   Execution time: {sum(metrics['avg_execution_times'].values()):.2f}s\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úÖ Integration Complete!\")\n",
        "print(\"‚úÖ Parallel execution working!\")\n",
        "print(\"‚úÖ Memory Bank storing analyses!\")\n",
        "print(f\"{'='*60}\")\n"
      ],
      "metadata": {
        "id": "p-ibPR6PzwbJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Test Reporter Agent\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from agents.reporter_agent import ReporterAgent\n",
        "from config import Config\n",
        "\n",
        "print(\"üöÄ Testing Reporter Agent\\n\")\n",
        "\n",
        "# Initialize services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "\n",
        "# ===== Step 1: Ingest =====\n",
        "print(\"Step 1: Loading data...\")\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "# ===== Step 2: Analyze =====\n",
        "print(\"\\nStep 2: Running analysis...\")\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "# ===== Step 3: Detect Anomalies =====\n",
        "print(\"\\nStep 3: Detecting anomalies...\")\n",
        "anomaly_agent = AnomalyAgent(session, logger)\n",
        "anomaly_results = anomaly_agent.detect_anomalies(df)\n",
        "\n",
        "# ===== Step 4: Generate Report =====\n",
        "print(\"\\nStep 4: Generating PDF report...\")\n",
        "reporter_agent = ReporterAgent(session, logger)\n",
        "report_result = reporter_agent.generate_report()\n",
        "\n",
        "# ===== Display Results =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"REPORT GENERATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\n‚úÖ Report Generated Successfully!\")\n",
        "print(f\"\\nüìÑ Report Details:\")\n",
        "print(f\"   Filename: {report_result['report_filename']}\")\n",
        "print(f\"   Path: {report_result['report_path']}\")\n",
        "print(f\"   Pages: {report_result['pages']}\")\n",
        "print(f\"   Generated: {report_result['generated_at']}\")\n",
        "\n",
        "print(f\"\\nüìä Report Contents:\")\n",
        "print(f\"   Dataset: {report_result['dataset']}\")\n",
        "print(f\"   Anomalies: {report_result['total_anomalies']}\")\n",
        "print(f\"   Insights: {report_result['total_insights']}\")\n",
        "print(f\"   Visualizations: {report_result['total_plots']}\")\n",
        "\n",
        "print(f\"\\n‚úÖ PDF Report successfully created!\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "# Check if file exists\n",
        "import os\n",
        "if os.path.exists(report_result['report_path']):\n",
        "    file_size = os.path.getsize(report_result['report_path']) / 1024\n",
        "    print(f\"\\nüì¶ File size: {file_size:.2f} KB\")\n",
        "    print(f\"‚úì Report is ready for download!\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Report file not found at: {report_result['report_path']}\")\n"
      ],
      "metadata": {
        "id": "cQNBe3n1Ed8-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 21: Integration Test - Full Pipeline\n",
        "\n",
        "# --- relative import for repo usage ---\n",
        "# Ensure the 'services' folder is placed in the same folder as this notebook in the repo.\n",
        "import os, sys\n",
        "# Add the repo root (not Drive) so Python can import 'services'\n",
        "repo_root = os.path.abspath(\".\")\n",
        "if repo_root not in sys.path:\n",
        "    sys.path.insert(0, repo_root)\n",
        "# Now import from services as usual\n",
        "from services.session_service import SessionService\n",
        "\n",
        "import pandas as pd\n",
        "from utils.logger import AgentLogger\n",
        "from services.memory_bank import MemoryBank\n",
        "from agents.ingest_agent import IngestAgent\n",
        "from agents.analysis_agent import AnalysisAgent\n",
        "from agents.anomaly_agent import AnomalyAgent\n",
        "from agents.reporter_agent import ReporterAgent\n",
        "from tools.alert_tool import AlertTool, AlertSeverity\n",
        "from config import Config\n",
        "import time\n",
        "\n",
        "print(\"üöÄ END-TO-END INTEGRATION TEST\\n\")\n",
        "print(\"=\"*60)\n",
        "print(\"DataSense AI - Full Multi-Agent Analysis Pipeline\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Initialize all services\n",
        "logger = AgentLogger(Config.LOGS_PATH)\n",
        "session = SessionService()\n",
        "memory = MemoryBank()\n",
        "alert_tool = AlertTool(logger)\n",
        "\n",
        "# ===== PHASE 1: DATA INGESTION =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 1: DATA INGESTION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_phase1 = time.time()\n",
        "\n",
        "ingest_agent = IngestAgent(session, logger)\n",
        "profile = ingest_agent.ingest_csv(f'{Config.BASE_PATH}/sample_data/sales.csv')\n",
        "df = ingest_agent.get_dataframe()\n",
        "\n",
        "phase1_time = time.time() - start_phase1\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"‚úÖ Dataset Loaded\",\n",
        "    f\"{df.shape[0]:,} rows √ó {df.shape[1]} columns loaded successfully\",\n",
        "    AlertSeverity.INFO,\n",
        "    {'quality': profile['quality_metrics']['overall_quality']}\n",
        ")\n",
        "\n",
        "# ===== PHASE 2: DATA ANALYSIS =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: DATA ANALYSIS (EDA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_phase2 = time.time()\n",
        "\n",
        "analysis_agent = AnalysisAgent(session, logger)\n",
        "analysis_results = analysis_agent.analyze(df)\n",
        "\n",
        "phase2_time = time.time() - start_phase2\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"‚úÖ Analysis Complete\",\n",
        "    f\"Generated {len(analysis_results['visualizations'])} plots, {len(analysis_results['insights'])} insights\",\n",
        "    AlertSeverity.INFO\n",
        ")\n",
        "\n",
        "# ===== PHASE 3: ANOMALY DETECTION (PARALLEL) =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 3: ANOMALY DETECTION (PARALLEL EXECUTION)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_phase3 = time.time()\n",
        "\n",
        "anomaly_agent = AnomalyAgent(session, logger)\n",
        "anomaly_results = anomaly_agent.detect_anomalies(df)\n",
        "\n",
        "phase3_time = time.time() - start_phase3\n",
        "\n",
        "# Alert for anomalies\n",
        "high_severity = [a for a in anomaly_results['anomalies'] if a.get('severity') in ['critical', 'high']]\n",
        "if high_severity:\n",
        "    alert_tool.send_alert(\n",
        "        \"‚ö†Ô∏è  High-Severity Anomalies\",\n",
        "        f\"Detected {len(high_severity)} critical/high anomalies\",\n",
        "        AlertSeverity.WARNING\n",
        "    )\n",
        "\n",
        "# ===== PHASE 4: REPORT GENERATION =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 4: REPORT GENERATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "start_phase4 = time.time()\n",
        "\n",
        "reporter_agent = ReporterAgent(session, logger)\n",
        "report_result = reporter_agent.generate_report()\n",
        "\n",
        "phase4_time = time.time() - start_phase4\n",
        "\n",
        "# Store in memory\n",
        "analysis_summary = {\n",
        "    'shape': df.shape,\n",
        "    'columns': df.columns.tolist(),\n",
        "    'anomalies_count': len(anomaly_results['anomalies']),\n",
        "    'quality_score': profile['quality_metrics']['completeness_score'],\n",
        "    'key_findings': [\n",
        "        f\"Detected {len(anomaly_results['anomalies'])} anomalies\",\n",
        "        f\"Quality: {profile['quality_metrics']['overall_quality']}\",\n",
        "        f\"Generated {len(analysis_results['visualizations'])} visualizations\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "memory_id = memory.store_analysis('sales.csv', analysis_summary)\n",
        "\n",
        "alert_tool.send_alert(\n",
        "    \"‚úÖ Report Generated\",\n",
        "    f\"Created {report_result['pages']}-page PDF report\",\n",
        "    AlertSeverity.INFO,\n",
        "    {'filename': report_result['report_filename']}\n",
        ")\n",
        "\n",
        "# ===== FINAL SUMMARY =====\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL SUMMARY - ALL PHASES COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "total_time = phase1_time + phase2_time + phase3_time + phase4_time\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  EXECUTION TIMES:\")\n",
        "print(f\"   Phase 1 (Ingest):        {phase1_time:.2f}s\")\n",
        "print(f\"   Phase 2 (Analysis):      {phase2_time:.2f}s\")\n",
        "print(f\"   Phase 3 (Anomalies):     {phase3_time:.2f}s\")\n",
        "print(f\"   Phase 4 (Report):        {phase4_time:.2f}s\")\n",
        "print(f\"   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
        "print(f\"   TOTAL:                   {total_time:.2f}s\")\n",
        "\n",
        "# Session summary\n",
        "session_summary = session.get_session_summary()\n",
        "\n",
        "print(f\"\\nüìä ANALYSIS SUMMARY:\")\n",
        "print(f\"   Session ID: {session_summary['session_id']}\")\n",
        "print(f\"   Dataset: {session_summary['dataset_name']} ({session_summary['dataset_shape']})\")\n",
        "\n",
        "print(f\"\\nüìà FINDINGS:\")\n",
        "print(f\"   Total Insights: {session_summary['total_insights']}\")\n",
        "print(f\"   Total Anomalies: {session_summary['total_anomalies']}\")\n",
        "print(f\"   Total Plots: {session_summary['total_plots']}\")\n",
        "\n",
        "print(f\"\\n‚ö†Ô∏è  SEVERITY BREAKDOWN:\")\n",
        "for severity, count in session_summary['anomalies_by_severity'].items():\n",
        "    if count > 0:\n",
        "        print(f\"   {severity.upper()}: {count}\")\n",
        "\n",
        "print(f\"\\nü§ñ AGENTS EXECUTED:\")\n",
        "for agent in session_summary['agents_executed']:\n",
        "    print(f\"   ‚úì {agent}\")\n",
        "\n",
        "print(f\"\\nüìÑ REPORT DETAILS:\")\n",
        "print(f\"   Filename: {report_result['report_filename']}\")\n",
        "print(f\"   Pages: {report_result['pages']}\")\n",
        "print(f\"   Path: {report_result['report_path']}\")\n",
        "\n",
        "print(f\"\\nüíæ MEMORY BANK:\")\n",
        "print(f\"   Analysis ID: {memory_id}\")\n",
        "print(f\"   Total Stored: {len(memory.get_all_memories())}\")\n",
        "\n",
        "# Metrics\n",
        "metrics = logger.get_metrics_summary()\n",
        "\n",
        "print(f\"\\nüìä PERFORMANCE METRICS:\")\n",
        "print(f\"   Total Agent Calls: {metrics['total_agent_calls']}\")\n",
        "print(f\"   Total Tool Calls: {metrics['total_tool_calls']}\")\n",
        "print(f\"   Total Errors: {metrics['total_errors']}\")\n",
        "\n",
        "print(f\"\\nüîî ALERTS SENT: {len(alert_tool.get_all_alerts())}\")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"‚úÖ COMPLETE END-TO-END PIPELINE SUCCESSFUL!\")\n",
        "print(\"‚úÖ All 4 agents working together perfectly!\")\n",
        "print(\"‚úÖ Professional PDF report generated!\")\n",
        "print(f\"{'='*60}\\n\")\n"
      ],
      "metadata": {
        "id": "53a8I5m6qkgM",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Final Test Run\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "test_files = [\n",
        "    'test_ingest_agent.py',\n",
        "    'test_analysis_agent.py',\n",
        "    'test_anomaly_agent.py',\n",
        "    'test_end_to_end.py'\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"RUNNING ALL TESTS (FINAL)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "passed = 0\n",
        "failed = 0\n",
        "\n",
        "for test_file in test_files:\n",
        "    print(f\"\\nRunning: {test_file}\")\n",
        "\n",
        "    result = subprocess.run(\n",
        "        [sys.executable, f'/content/drive/MyDrive/DataSense_AI_Agent/tests/{test_file}'],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=120\n",
        "    )\n",
        "\n",
        "    if result.returncode == 0:\n",
        "        passed += 1\n",
        "        print(f\"‚úÖ PASSED\")\n",
        "    else:\n",
        "        failed += 1\n",
        "        print(f\"‚ùå FAILED\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"RESULTS: {passed} PASSED, {failed} FAILED out of {len(test_files)}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if failed == 0:\n",
        "    print(\"\\nüéâ ALL TESTS PASSED! Ready for Day 6!\")\n"
      ],
      "metadata": {
        "id": "ngkmJFNGg_Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 24: Evaluation Framework\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/DataSense_AI_Agent')\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "class EvaluationFramework:\n",
        "    \"\"\"\n",
        "    Framework for collecting and analyzing human feedback\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.evaluations = []\n",
        "\n",
        "    def add_evaluation(self,\n",
        "                      dataset_name: str,\n",
        "                      dataset_clarity: int,  # 1-5\n",
        "                      insight_usefulness: int,  # 1-5\n",
        "                      anomaly_relevance: int,  # 1-5\n",
        "                      report_clarity: int,  # 1-5\n",
        "                      overall_satisfaction: int,  # 1-5\n",
        "                      comments: str = \"\"):\n",
        "        \"\"\"\n",
        "        Record one person's evaluation\n",
        "\n",
        "        Args:\n",
        "            dataset_name: Name of dataset tested\n",
        "            dataset_clarity: 1-5 rating\n",
        "            insight_usefulness: 1-5 rating\n",
        "            anomaly_relevance: 1-5 rating\n",
        "            report_clarity: 1-5 rating\n",
        "            overall_satisfaction: 1-5 rating\n",
        "            comments: Optional feedback\n",
        "        \"\"\"\n",
        "\n",
        "        evaluation = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'dataset': dataset_name,\n",
        "            'dataset_clarity': dataset_clarity,\n",
        "            'insight_usefulness': insight_usefulness,\n",
        "            'anomaly_relevance': anomaly_relevance,\n",
        "            'report_clarity': report_clarity,\n",
        "            'overall_satisfaction': overall_satisfaction,\n",
        "            'comments': comments\n",
        "        }\n",
        "\n",
        "        self.evaluations.append(evaluation)\n",
        "        print(f\"‚úÖ Evaluation recorded for {dataset_name}\")\n",
        "\n",
        "    def get_summary(self) -> dict:\n",
        "        \"\"\"\n",
        "        Calculate statistics from all evaluations\n",
        "        \"\"\"\n",
        "        if not self.evaluations:\n",
        "            return {'error': 'No evaluations recorded'}\n",
        "\n",
        "        df = pd.DataFrame(self.evaluations)\n",
        "\n",
        "        # Calculate averages\n",
        "        metrics = [\n",
        "            'dataset_clarity',\n",
        "            'insight_usefulness',\n",
        "            'anomaly_relevance',\n",
        "            'report_clarity',\n",
        "            'overall_satisfaction'\n",
        "        ]\n",
        "\n",
        "        summary = {\n",
        "            'total_evaluations': len(self.evaluations),\n",
        "            'average_scores': {},\n",
        "            'min_scores': {},\n",
        "            'max_scores': {}\n",
        "        }\n",
        "\n",
        "        for metric in metrics:\n",
        "            scores = df[metric].values\n",
        "            summary['average_scores'][metric] = round(sum(scores) / len(scores), 2)\n",
        "            summary['min_scores'][metric] = min(scores)\n",
        "            summary['max_scores'][metric] = max(scores)\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def print_report(self):\n",
        "        \"\"\"Print evaluation report\"\"\"\n",
        "        summary = self.get_summary()\n",
        "\n",
        "        if 'error' in summary:\n",
        "            print(summary['error'])\n",
        "            return\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"HUMAN EVALUATION REPORT\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        print(f\"\\nüìä Total Evaluations: {summary['total_evaluations']}\")\n",
        "\n",
        "        print(f\"\\nüìà Average Scores (out of 5):\")\n",
        "\n",
        "        for metric, avg_score in summary['average_scores'].items():\n",
        "            # Visual bar\n",
        "            filled = int(avg_score)\n",
        "            bar = \"‚ñà\" * filled + \"‚ñë\" * (5 - filled)\n",
        "\n",
        "            metric_label = metric.replace('_', ' ').title()\n",
        "            print(f\"   {metric_label:.<30} {bar} {avg_score:.1f}/5\")\n",
        "\n",
        "        # Overall average\n",
        "        overall_avg = summary['average_scores']['overall_satisfaction']\n",
        "        print(f\"\\n   {'Overall Satisfaction':.<30} {overall_avg:.1f}/5\")\n",
        "\n",
        "        print(f\"\\n{'='*60}\")\n",
        "\n",
        "\n",
        "# Example usage: Add sample evaluations\n",
        "print(\"Creating Evaluation Framework...\\n\")\n",
        "\n",
        "evaluator = EvaluationFramework()\n",
        "\n",
        "# Simulated evaluations\n",
        "sample_evaluations = [\n",
        "    ('sales.csv', 5, 4, 4, 5, 4, 'Great tool, very clear insights'),\n",
        "    ('customer_data.csv', 4, 4, 3, 4, 4, 'Good but more granular anomalies would help'),\n",
        "    ('transactions.csv', 5, 5, 4, 5, 5, 'Excellent! Would definitely use again'),\n",
        "]\n",
        "\n",
        "print(\"Recording sample evaluations...\\n\")\n",
        "for dataset, clarity, usefulness, relevance, report, overall, comment in sample_evaluations:\n",
        "    evaluator.add_evaluation(\n",
        "        dataset_name=dataset,\n",
        "        dataset_clarity=clarity,\n",
        "        insight_usefulness=usefulness,\n",
        "        anomaly_relevance=relevance,\n",
        "        report_clarity=report,\n",
        "        overall_satisfaction=overall,\n",
        "        comments=comment\n",
        "    )\n",
        "\n",
        "# Print report\n",
        "evaluator.print_report()\n",
        "\n",
        "# Export to CSV\n",
        "evaluations_df = pd.DataFrame(evaluator.evaluations)\n",
        "evaluations_df.to_csv(\n",
        "    '/content/drive/MyDrive/DataSense_AI_Agent/outputs/evaluations.csv',\n",
        "    index=False\n",
        ")\n",
        "print(\"\\n‚úÖ Evaluations saved to: outputs/evaluations.csv\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w2lJgnUANKGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wKGb2XpYKuuH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gPS60OPGRusy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}